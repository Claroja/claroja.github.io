const n=JSON.parse(`{"key":"v-2308c0eb","path":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/scikit/dataSplit.html","title":"dataSplit","lang":"zh-CN","frontmatter":{"description":"dataSplit single labels import pandas as pd from datasets import Dataset,DatasetDict from transformers import AutoTokenizer from torch.utils.data import DataLoader from torch.optim import AdamW import torch from transformers import AutoModelForSequenceClassification from sklearn.model_selection import train_test_split from sklearn import preprocessing from sklearn.metrics import accuracy_score ## 1. create test data df = pd.DataFrame({ \\"text\\":[\\"text0\\",\\"text1\\",\\"text2\\",\\"text3\\",\\"text4\\",\\"text5\\",\\"text6\\"], \\"target\\":[\\"a\\",\\"b\\",\\"b\\",\\"a\\",\\"b\\",\\"a\\",\\"a\\"] }) ## 2. label the target labels = df[\\"target\\"].value_counts().index.tolist() le = preprocessing.LabelEncoder() le.fit(labels) df[\\"labels\\"] = le.transform(df[\\"target\\"]) text_column_name = \\"text\\" labels_column_name = \\"labels\\" ## 3. split into train and test df_with_index = df.reset_index() X_train, X_test, y_train, y_test = train_test_split(df_with_index[[\\"index\\",text_column_name]].values,df_with_index[labels_column_name].values,test_size=0.5) df_with_index.loc[df_with_index[\\"index\\"].isin(X_train[:,0]),\\"splitmark\\"] = \\"test\\" df_with_index.loc[df_with_index[\\"index\\"].isin(X_test[:,0]), \\"splitmark\\"] = \\"train\\" df_with_splitmark = df_with_index.set_index(\\"index\\") ## 4. tokenize and create dataloader dataset_train = Dataset.from_pandas(df_with_splitmark.loc[df_with_index[\\"splitmark\\"]==\\"train\\",[\\"text\\",\\"labels\\"]]) dataset_test = Dataset.from_pandas(df_with_splitmark.loc[df_with_index[\\"splitmark\\"]==\\"test\\",[\\"text\\",\\"labels\\"]]) datadict = DatasetDict({\\"train\\":dataset_train,\\"test\\":dataset_test}) tokenizer = AutoTokenizer.from_pretrained(\\"./pretrained_model/distilbert-base-uncased\\") tokenized_datadict = datadict.map(lambda x:tokenizer(x[text_column_name], padding='max_length', truncation=True, max_length=512,), batched=True,remove_columns = [\\"index\\",text_column_name]) tokenized_datadict.set_format(\\"torch\\") train_dataloader = DataLoader(tokenized_datadict[\\"train\\"], shuffle=True, batch_size=1) test_dataloader = DataLoader(tokenized_datadict[\\"test\\"], batch_size=1)","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/scikit/dataSplit.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"dataSplit"}],["meta",{"property":"og:description","content":"dataSplit single labels import pandas as pd from datasets import Dataset,DatasetDict from transformers import AutoTokenizer from torch.utils.data import DataLoader from torch.optim import AdamW import torch from transformers import AutoModelForSequenceClassification from sklearn.model_selection import train_test_split from sklearn import preprocessing from sklearn.metrics import accuracy_score ## 1. create test data df = pd.DataFrame({ \\"text\\":[\\"text0\\",\\"text1\\",\\"text2\\",\\"text3\\",\\"text4\\",\\"text5\\",\\"text6\\"], \\"target\\":[\\"a\\",\\"b\\",\\"b\\",\\"a\\",\\"b\\",\\"a\\",\\"a\\"] }) ## 2. label the target labels = df[\\"target\\"].value_counts().index.tolist() le = preprocessing.LabelEncoder() le.fit(labels) df[\\"labels\\"] = le.transform(df[\\"target\\"]) text_column_name = \\"text\\" labels_column_name = \\"labels\\" ## 3. split into train and test df_with_index = df.reset_index() X_train, X_test, y_train, y_test = train_test_split(df_with_index[[\\"index\\",text_column_name]].values,df_with_index[labels_column_name].values,test_size=0.5) df_with_index.loc[df_with_index[\\"index\\"].isin(X_train[:,0]),\\"splitmark\\"] = \\"test\\" df_with_index.loc[df_with_index[\\"index\\"].isin(X_test[:,0]), \\"splitmark\\"] = \\"train\\" df_with_splitmark = df_with_index.set_index(\\"index\\") ## 4. tokenize and create dataloader dataset_train = Dataset.from_pandas(df_with_splitmark.loc[df_with_index[\\"splitmark\\"]==\\"train\\",[\\"text\\",\\"labels\\"]]) dataset_test = Dataset.from_pandas(df_with_splitmark.loc[df_with_index[\\"splitmark\\"]==\\"test\\",[\\"text\\",\\"labels\\"]]) datadict = DatasetDict({\\"train\\":dataset_train,\\"test\\":dataset_test}) tokenizer = AutoTokenizer.from_pretrained(\\"./pretrained_model/distilbert-base-uncased\\") tokenized_datadict = datadict.map(lambda x:tokenizer(x[text_column_name], padding='max_length', truncation=True, max_length=512,), batched=True,remove_columns = [\\"index\\",text_column_name]) tokenized_datadict.set_format(\\"torch\\") train_dataloader = DataLoader(tokenized_datadict[\\"train\\"], shuffle=True, batch_size=1) test_dataloader = DataLoader(tokenized_datadict[\\"test\\"], batch_size=1)"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-27T13:17:01.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2023-11-27T13:17:01.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"dataSplit\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-11-27T13:17:01.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"single labels","slug":"single-labels","link":"#single-labels","children":[]},{"level":2,"title":"multi labels","slug":"multi-labels","link":"#multi-labels","children":[]}],"git":{"createdTime":1701091021000,"updatedTime":1701091021000,"contributors":[{"name":"claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.45,"words":436},"filePathRelative":"机器学习/scikit/dataSplit.md","localizedDate":"2023年11月27日","excerpt":"<h1> dataSplit</h1>\\n<h2> single labels</h2>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> pandas <span class=\\"token keyword\\">as</span> pd\\n<span class=\\"token keyword\\">from</span> datasets <span class=\\"token keyword\\">import</span> Dataset<span class=\\"token punctuation\\">,</span>DatasetDict\\n<span class=\\"token keyword\\">from</span> transformers <span class=\\"token keyword\\">import</span> AutoTokenizer\\n<span class=\\"token keyword\\">from</span> torch<span class=\\"token punctuation\\">.</span>utils<span class=\\"token punctuation\\">.</span>data <span class=\\"token keyword\\">import</span> DataLoader\\n<span class=\\"token keyword\\">from</span> torch<span class=\\"token punctuation\\">.</span>optim <span class=\\"token keyword\\">import</span> AdamW\\n<span class=\\"token keyword\\">import</span> torch\\n<span class=\\"token keyword\\">from</span> transformers <span class=\\"token keyword\\">import</span> AutoModelForSequenceClassification\\n<span class=\\"token keyword\\">from</span> sklearn<span class=\\"token punctuation\\">.</span>model_selection <span class=\\"token keyword\\">import</span> train_test_split\\n<span class=\\"token keyword\\">from</span> sklearn <span class=\\"token keyword\\">import</span> preprocessing\\n<span class=\\"token keyword\\">from</span> sklearn<span class=\\"token punctuation\\">.</span>metrics <span class=\\"token keyword\\">import</span> accuracy_score\\n\\n\\n\\n<span class=\\"token comment\\">## 1. create test data</span>\\ndf <span class=\\"token operator\\">=</span> pd<span class=\\"token punctuation\\">.</span>DataFrame<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">{</span>\\n    <span class=\\"token string\\">\\"text\\"</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"text0\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text1\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text2\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text3\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text4\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text5\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"text6\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span>\\n    <span class=\\"token string\\">\\"target\\"</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"a\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"b\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"b\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"a\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"b\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"a\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"a\\"</span><span class=\\"token punctuation\\">]</span>\\n<span class=\\"token punctuation\\">}</span><span class=\\"token punctuation\\">)</span>\\n\\n<span class=\\"token comment\\">## 2. label the target</span>\\n\\nlabels <span class=\\"token operator\\">=</span> df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"target\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>value_counts<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>index<span class=\\"token punctuation\\">.</span>tolist<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\nle <span class=\\"token operator\\">=</span> preprocessing<span class=\\"token punctuation\\">.</span>LabelEncoder<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\nle<span class=\\"token punctuation\\">.</span>fit<span class=\\"token punctuation\\">(</span>labels<span class=\\"token punctuation\\">)</span>\\ndf<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> le<span class=\\"token punctuation\\">.</span>transform<span class=\\"token punctuation\\">(</span>df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"target\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\ntext_column_name <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"text\\"</span>\\nlabels_column_name <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"labels\\"</span>\\n\\n<span class=\\"token comment\\">## 3. split into train and test</span>\\ndf_with_index <span class=\\"token operator\\">=</span> df<span class=\\"token punctuation\\">.</span>reset_index<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\nX_train<span class=\\"token punctuation\\">,</span> X_test<span class=\\"token punctuation\\">,</span> y_train<span class=\\"token punctuation\\">,</span> y_test <span class=\\"token operator\\">=</span> train_test_split<span class=\\"token punctuation\\">(</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">,</span>text_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>values<span class=\\"token punctuation\\">,</span>df_with_index<span class=\\"token punctuation\\">[</span>labels_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>values<span class=\\"token punctuation\\">,</span>test_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.5</span><span class=\\"token punctuation\\">)</span>\\ndf_with_index<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>isin<span class=\\"token punctuation\\">(</span>X_train<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">,</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"test\\"</span>\\ndf_with_index<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>isin<span class=\\"token punctuation\\">(</span>X_test<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">,</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span> <span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"train\\"</span>\\ndf_with_splitmark <span class=\\"token operator\\">=</span> df_with_index<span class=\\"token punctuation\\">.</span>set_index<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">)</span>\\n\\n\\n<span class=\\"token comment\\">## 4. tokenize and create dataloader</span>\\ndataset_train <span class=\\"token operator\\">=</span> Dataset<span class=\\"token punctuation\\">.</span>from_pandas<span class=\\"token punctuation\\">(</span>df_with_splitmark<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">==</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"text\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\ndataset_test <span class=\\"token operator\\">=</span> Dataset<span class=\\"token punctuation\\">.</span>from_pandas<span class=\\"token punctuation\\">(</span>df_with_splitmark<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">==</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"text\\"</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\ndatadict <span class=\\"token operator\\">=</span> DatasetDict<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">{</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">:</span>dataset_train<span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">:</span>dataset_test<span class=\\"token punctuation\\">}</span><span class=\\"token punctuation\\">)</span>\\ntokenizer <span class=\\"token operator\\">=</span> AutoTokenizer<span class=\\"token punctuation\\">.</span>from_pretrained<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"./pretrained_model/distilbert-base-uncased\\"</span><span class=\\"token punctuation\\">)</span>\\ntokenized_datadict <span class=\\"token operator\\">=</span> datadict<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">map</span><span class=\\"token punctuation\\">(</span><span class=\\"token keyword\\">lambda</span> x<span class=\\"token punctuation\\">:</span>tokenizer<span class=\\"token punctuation\\">(</span>x<span class=\\"token punctuation\\">[</span>text_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> padding<span class=\\"token operator\\">=</span><span class=\\"token string\\">'max_length'</span><span class=\\"token punctuation\\">,</span> truncation<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span> max_length<span class=\\"token operator\\">=</span><span class=\\"token number\\">512</span><span class=\\"token punctuation\\">,</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span>\\n                                batched<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span>remove_columns <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">,</span>text_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\ntokenized_datadict<span class=\\"token punctuation\\">.</span>set_format<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"torch\\"</span><span class=\\"token punctuation\\">)</span>\\ntrain_dataloader <span class=\\"token operator\\">=</span> DataLoader<span class=\\"token punctuation\\">(</span>tokenized_datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> shuffle<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span> batch_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span>\\ntest_dataloader <span class=\\"token operator\\">=</span> DataLoader<span class=\\"token punctuation\\">(</span>tokenized_datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> batch_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}`);export{n as data};
