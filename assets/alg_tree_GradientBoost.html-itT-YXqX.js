import{_ as s}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as i,e as t,a as e}from"./app-XqA98pG8.js";const o="/assets/1-VNCwDjjE.png",r="/assets/2-9DiWLqJX.png",n="/assets/3-vQJ0GNhr.gif",d="/assets/4-tVdLLTpJ.gif",l="/assets/5-X4bf8nmP.png",h="/assets/6-bYQsv06s.gif",c="/assets/7-FCidKtb5.png",g={},m=t('<h1 id="tree-gradientboost" tabindex="-1"><a class="header-anchor" href="#tree-gradientboost" aria-hidden="true">#</a> tree_GradientBoost</h1><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><ol><li>Gradient Boost最初会做一个单独的叶子节点，而不是tree或者stump，这个叶子节点随机猜测目标值，第一次预测是平均值</li><li>将每个样本实际值，减去预测值，得到每个样本的pseudo residual</li><li>然后我们建立一棵树分别预测每个样本的pseudo residual</li><li>现在预测值就是最初的平均值加上这棵树的预测值（我们给这棵树的预测值加上一个权重，就是学习速率）</li><li>如此循环下去</li></ol><h2 id="流程" tabindex="-1"><a class="header-anchor" href="#流程" aria-hidden="true">#</a> 流程</h2><p>Specifically, we will use this data where we have the <code>Height</code> measurements from 6 people, their favorite colors, their genders and their weights.</p><p><code>Gradient Boost</code> starts by making a single leaf, instead of a tree or stump. This leaf represents an initial guess for the <code>Weights</code> of all of the samples.When trying to predict a continuous value like weight, the first guess is the average value.Then <code>gradient boost</code> builds a tree.Like <code>AdaBoost</code>,this tree is based on the errors made by the previous tree.But unlike <code>Adaboost</code>, this tree is usually larger than a stump. That said, <code>Gradient Boost</code> still restricts the size of the tree. In practice, people often set the maximum number of leaves to be between 8 and 32.Thus, like <code>AdaBoost</code>, <code>Gradient Boost</code> builds fixed sized trees based on the previous tree&#39;s errors, but unlike <code>AdaBoost</code>, each tree can be larger than a stump. Then <code>Gradient Boost</code> builds another tree based on the errors made by the previous tree and then it scales the tree and <code>Gradient Boost</code> continues to build trees in this fashion until it has made the number of trees you ask for, or additional trees fail to improve the fit. Let&#39;s see how the most common <code>Gradient Boost</code> configuration would use this training data to predict weight. The first thing we do is calculate the average weight. This is the first attempt at predicting everyone&#39;s weight.In other words, if we stopped right now, we would predict that everyone weighted 71.2 kg. <img src="'+o+'" alt="" loading="lazy"> The next thing we do is build a tree based on the errors from the first tree.The errors the previous tree made are the differences between the observed weights and the predicted weight, 71.2.So let&#39;s start by plugging in 71.2 for the predicted weight and then plug in the first observed weight and do the math. And save the difference, which is called a <code>Pseudo Residual</code>, in a column.</p>',6),p=e("p",{class:"katex-block"},[e("span",{class:"katex-display"},[e("span",{class:"katex"},[e("span",{class:"katex-mathml"},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[e("semantics",null,[e("mrow",null,[e("mn",null,"88"),e("mo",null,"−"),e("mn",null,"71.2"),e("mo",null,"="),e("mn",null,"16.8")]),e("annotation",{encoding:"application/x-tex"}," 88-71.2=16.8 ")])])]),e("span",{class:"katex-html","aria-hidden":"true"},[e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.7278em","vertical-align":"-0.0833em"}}),e("span",{class:"mord"},"88"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),e("span",{class:"mbin"},"−"),e("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6444em"}}),e("span",{class:"mord"},"71.2"),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),e("span",{class:"mrel"},"="),e("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),e("span",{class:"base"},[e("span",{class:"strut",style:{height:"0.6444em"}}),e("span",{class:"mord"},"16.8")])])])])],-1),u=t('<p>NOTE:The term <code>Pseudo Residual</code> is based on <code>linear regression</code>, where difference between observed values and the predicted values results in residuals. The <code>Pseudo</code> part of <code>Pseudo Residual</code> is a reminder that we are doing <code>Gradient Boost</code> not <code>Linear Regression</code>. Now we do the same thing for the remaining weights. <img src="'+r+'" alt="" loading="lazy"> Now we will build a tree,using <code>Height</code>,<code>Favorite color</code> and <code>Gender</code>.If it seems strange to predict the residuals instead of the original weights. Remember, in this example, we are only allowing up to four leaves,but when using a larger dataset, it is common to allow anywhere from 8 to 32. By restricting the total number of leaves, we get fewer leaves than residuals. <img src="'+n+'" alt="" loading="lazy"> Now we can combine the original leaf with the new tree to make a new prediction of an individual&#39;s weight from the training data. We start with the initial prediction, 71.2. Then we run the data down the tree and we get 16.8, so the predict weight = 71.2 + 16.8 = 88 which is the same as the observed weight. <img src="'+d+'" alt="" loading="lazy"> The model fits the training data too well.In other words, we have low bias, but probably very high variance.<code>Gradient Boost</code> deals with this problem by using a <code>Learning Rate</code> to scale the contribution from the new tree. The <code>Learning Rate</code> is a value between 0 and 1.In this case, we&#39;ll set the <code>Learning Rate</code> to 0.1. Now the predicted weight = 71.2 + (0.1 * 16.8) = 72.9. <img src="'+l+'" alt="" loading="lazy"> With the Learning Rate set to 0.1, the new prediction is not as good as it was before.But it&#39;s a little bit better than the prediction made with just the original leaf, which predicted that all samples would weight 71.2. In other words, scaling the tree by the <code>Learning Rate</code> results in a small step in the right direction.According to the dude that invented <code>Gradient Boost</code>,Jerome Friedman, empirical evidence shows that taking lots of small steps in the right direction results in better predictions with a testing dataset, i.e. lower variance. So let&#39;s build another tree so we can take another small step in the right direction.Just like before, we calculate the <code>Pseudo residuals</code>, between the observed weights and our latest predictions. <img src="'+h+'" alt="" loading="lazy"> as above, we can continue to build tree with <code>Learning Rate = 0.1</code>, every tree, will fit the residuals the previous tree made.</p><p>Remember, the left residuals is when we just used a single leaf to <code>predict weight</code>. The middle residuals is after we added the first tree to the prediction. The right residuals is after we added the second tree to the prediction.Each time we add a tree to the prediction, the residuals get smaller.So we&#39;ve taken another small step towards making good predictions. <img src="'+c+'" alt="" loading="lazy"></p><p>refs: https://www.youtube.com/watch?v=3CC4N4z3GJc</p>',3),w=[m,p,u];function f(b,v){return a(),i("div",null,w)}const k=s(g,[["render",f],["__file","alg_tree_GradientBoost.html.vue"]]);export{k as default};
