import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as t,c as e,e as a}from"./app-jdLxCr9I.js";const l="/assets/聚类分析1-GHX8-hhj.png",s="/assets/聚类分析2-KFCA21DL.png",n="/assets/聚类分析3-RLFtAx0I.png",r="/assets/聚类分析4-GGwGYnfi.png",o="/assets/聚类分析5-BRsiVOe9.png",c="/assets/聚类分析6-6asstOBC.png",d="/assets/聚类分析7-yuZYUihP.png",g="/assets/聚类分析8-ayg5UfKv.png",p="/assets/聚类分析9-vR-gB755.png",h="/assets/聚类分析10-whjOt5F4.png",u="/assets/聚类分析11-APj3Oehd.png",m="/assets/聚类分析12-ZdZDl6j3.png",f="/assets/聚类分析13-ZM7KIDPA.png",x="/assets/聚类分析14-hA3vjDM2.png",_="/assets/聚类分析15-pIkxa84i.png",b="/assets/聚类分析16-I8yQK8eE.png",y="/assets/聚类分析17-CmPFC9nH.png",S="/assets/聚类分析18-b_HxD8He.png",z="/assets/聚类分析19-HVEkS9eo.png",v="/assets/聚类分析20-0ZDyYnzY.png",M="/assets/聚类分析21-1dj-8X4Z.png",C="/assets/聚类分析22-3ylwlzO5.png",A="/assets/聚类分析23-2zarqMjh.png",w="/assets/聚类分析24-kpWnSWhq.png",k="/assets/聚类分析25--ZwLFCbL.png",E="/assets/聚类分析26-0gNmAGi7.png",D="/assets/聚类分析27-tlxxKcYT.png",K="/assets/聚类分析28-tcGrFvIv.png",O="/assets/聚类分析29-dWbfhET1.png",B="/assets/聚类分析30-z055PENw.png",N="/assets/聚类分析31-AZ7CjA7K.png",I="/assets/聚类分析32-8bM_6POi.png",R="/assets/聚类分析33-GjtNkcv5.png",j="/assets/聚类分析34-nc6YTxzk.png",P="/assets/聚类分析35-pmI1uhxh.png",q="/assets/聚类分析36-t5D45H7I.png",T="/assets/聚类分析38-f3E03WqG.png",F="/assets/聚类分析39-_tjX2xWG.png",L="/assets/聚类分析40-8IfJI9Oe.png",H="/assets/聚类分析41-F2jb3O-D.png",W="/assets/聚类分析42-hth38Juw.png",V="/assets/聚类分析43-y94LYiLO.png",G="/assets/聚类分析44-bIYtU7-s.png",Y="/assets/聚类分析45-qkusFqor.png",Z="/assets/聚类分析46-iOwLQ4il.png",U="/assets/聚类分析47-0QIUqS1A.png",J="/assets/聚类分析48-ZeNOYJkb.png",X="/assets/聚类分析49-LApSrAPF.png",Q="/assets/聚类分析50-j9ljqdKb.png",$="/assets/聚类分析51-tLsefVzg.png",ii="/assets/聚类分析52-rprYMBEE.png",ti="/assets/聚类分析53-c9Voibpg.png",ei="/assets/聚类分析54-ScXESvv7.png",ai="/assets/聚类分析55--gV6GTEU.png",li={},si=a('<ol><li>聚类的概念</li><li>相似性的衡量 <ol><li>二元变量的相似性衡量</li><li>混合类别型变量与数值型变量的相似性衡量</li></ol></li><li>距离的计算 <ol><li>Manhattan Distance / City-Block Distance（曼哈顿距离/城市街区距离）</li><li>Euclidean Distance（欧氏距离）</li></ol></li><li>聚类算法的分类 <ol><li>Exclusive vs. Non-Exclusive(Overlapping)的聚类算法</li><li>层次聚类, 划分聚类, 模糊聚类, 密度聚类</li></ol></li></ol><h1 id="聚类算法分类" tabindex="-1"><a class="header-anchor" href="#聚类算法分类" aria-hidden="true">#</a> 聚类算法分类</h1><ol><li>层次聚类算法：单一链结法、完全链结法、平均链结法、中心法、Ward&#39;s 法。</li><li>划分聚类法：K-Means 法、K-Medoids 法、Kohonen Self-Organizing Maps(SOM)法、两步法。</li><li>模糊聚类：EM 算法。</li><li>密度聚类：密度聚类算法(DBSCAN)。</li><li>群数的判断：R-Squared(R2)、Semi-Parial R-Squared、Root-Mean-Square Standard Deviation(RMSSTD)、轮廓系数(Silhouette Coefficient)。</li></ol><h2 id="聚类概念" tabindex="-1"><a class="header-anchor" href="#聚类概念" aria-hidden="true">#</a> 聚类概念</h2><ul><li>In cluster analysis, there is no pre-classified data and no distinction between dependent and independent data.</li><li>Instead, cluster analysis is a process to group similar objects together.</li><li>All members in one cluster are similar to each other and different from the members of other clusters, according to some similarity metric. <ul><li>同一集群内成员的相似性要愈高愈好，而不同集群间成员的相异性则要愈高愈好。</li></ul></li></ul><p>Customer Segmentation</p><p><img src="'+l+'" alt="alt text" loading="lazy"><img src="'+s+'" alt="text" loading="lazy"><img src="'+n+'" alt="text" loading="lazy"><img src="'+r+'" alt="text" loading="lazy"><img src="'+o+'" alt="text" loading="lazy"><img src="'+c+'" alt="text" loading="lazy"></p><h2 id="相似性的衡量及距离的计算" tabindex="-1"><a class="header-anchor" href="#相似性的衡量及距离的计算" aria-hidden="true">#</a> 相似性的衡量及距离的计算</h2><figure><img src="'+d+'" alt="text" tabindex="0" loading="lazy"><figcaption>text</figcaption></figure><h2 id="coefficient-for-binary-variables" tabindex="-1"><a class="header-anchor" href="#coefficient-for-binary-variables" aria-hidden="true">#</a> Coefficient for Binary Variables</h2><figure><img src="'+g+'" alt="text" tabindex="0" loading="lazy"><figcaption>text</figcaption></figure><p>Jaccard Coefficient:</p><figure><img src="'+p+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="attribute-typess-involved-in-cluster-analysis" tabindex="-1"><a class="header-anchor" href="#attribute-typess-involved-in-cluster-analysis" aria-hidden="true">#</a> Attribute Typess Involved in Cluster Analysis</h2><ul><li><p>Interval Variables</p><ul><li>An interval variable contains continuous measurements (e.g., height, weight, temperature, cost, etc.) which follow a linear scale</li><li>It is essential that intervals keep the same importance throughout the scale</li></ul></li><li><p>Nominal Variables</p><ul><li>A nominal variable takes on more than two states. For example, the eye color of a person can be blue, brown, green or grey eyes</li><li>These states may be coded as 1,2,..., M, however their order and the interval between any two states do not have any meaning</li></ul></li><li><p>Ordinal Variables An ordinal variable takes on more than two states. For example, you may ask someone to convey his/her appreciation of some paintings in terms of the following categories: 1=detest, 2=dislike, 3=indifferent, 4=like and 5=admire. In an ordinal variable, their states are ordered in a meaningful sequence. However, the interval between any two consecutive states are not equally distanced.</p></li><li><p>Binary Variables Binary variables have only two possible states. For example, the gender of a person is either female or male.</p></li></ul><h2 id="曼哈顿距离" tabindex="-1"><a class="header-anchor" href="#曼哈顿距离" aria-hidden="true">#</a> 曼哈顿距离</h2><figure><img src="'+h+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+u+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+m+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+f+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p><img src="'+x+'" alt="alt text" loading="lazy"><img src="'+_+'" alt="alt text" loading="lazy"></p><h2 id="聚类算法分类-1" tabindex="-1"><a class="header-anchor" href="#聚类算法分类-1" aria-hidden="true">#</a> 聚类算法分类</h2><ol><li>Exclusive vs. Non-Exclusive (Overlapping) Clustering Methods - K-means vs. EM.</li><li>Hierarchical Clustering Methods vs. Partitioning Clustering Methods.</li></ol><ul><li><p>Hierarchical Clustering Methods.</p><ul><li>Single Linkage Method &amp; Complete Linkage Method.</li><li>Average Linkage Method &amp; Centroid Method.</li><li>Ward&#39;s Method.</li></ul></li><li><p>Partitioning Clustering Methods.</p><ul><li>K-Means.</li><li>K-Medoids (PAM,...).</li><li>Kohonen Self-Organizing Maps (SOM).</li><li>Two Step.</li></ul></li><li><p>Fuzzy Clustering Methods.</p><ul><li>EM (Expectation Maximization Clustering Method).</li></ul></li><li><p>Density-based Spatial Clustering Methods.</p><ul><li>DBSCAN (Density-Based Spatial Clustering of Applications with Noise).</li></ul></li></ul><h2 id="hierarchical-methods" tabindex="-1"><a class="header-anchor" href="#hierarchical-methods" aria-hidden="true">#</a> Hierarchical Methods</h2><figure><img src="'+b+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="单一连接法" tabindex="-1"><a class="header-anchor" href="#单一连接法" aria-hidden="true">#</a> 单一连接法</h2><figure><img src="'+y+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="完全连接法-complete-linkage-method" tabindex="-1"><a class="header-anchor" href="#完全连接法-complete-linkage-method" aria-hidden="true">#</a> 完全连接法(Complete Linkage Method)</h2><figure><img src="'+S+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="平均连接法-average-linkage-method" tabindex="-1"><a class="header-anchor" href="#平均连接法-average-linkage-method" aria-hidden="true">#</a> 平均连接法(Average Linkage Method)</h2><figure><img src="'+z+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="中心连接法-centroid-method" tabindex="-1"><a class="header-anchor" href="#中心连接法-centroid-method" aria-hidden="true">#</a> 中心连接法(Centroid Method)</h2><figure><img src="'+v+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="ward-s-method" tabindex="-1"><a class="header-anchor" href="#ward-s-method" aria-hidden="true">#</a> Ward&#39;s Method</h2><p><img src="'+M+'" alt="alt text" loading="lazy"><img src="'+C+'" alt="alt text" loading="lazy"><img src="'+A+'" alt="alt text" loading="lazy"></p><p>在大部分的研究中，以使用平均链接法及华德法（Ward&#39;s Method）较佳，而以单一连结法较差。 划分方法（Partitioning Methods）又优于层次方法（Hierarchical Methods）。 两步法（TwoStep）。</p><p>K-均值法（K-Means）→将数据切分成许多小群（例如 50 群）→层次聚类方法（Hierarchical Clustering Methods）→决定群数。</p><h2 id="群数的判断" tabindex="-1"><a class="header-anchor" href="#群数的判断" aria-hidden="true">#</a> 群数的判断</h2><ol><li>如果使用阶层式聚类分析法，在集结完成后，接下来就要判断应分成几个群集才算恰当. 一般可以用陡坡图来判断.</li><li>以前述使用华德法(Ward&#39;s Method)进行聚类分析的例子，各阶段SS系数分别为1.00、3.50、8.33、38.00，其陡坡图如下</li></ol><figure><img src="'+w+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="划分聚类算法" tabindex="-1"><a class="header-anchor" href="#划分聚类算法" aria-hidden="true">#</a> 划分聚类算法</h2><figure><img src="'+k+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+E+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+D+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>Advantage</p><ul><li><p>Fast Disadvantages</p></li><li><p>Sensitive to Outliers. ✨可以做离群值检测</p></li><li><p>Not an Optimal Solution.</p><figure><img src="'+K+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure></li><li><p>Can just handle numerical data.</p></li><li><p>中心点的选取关系着分类的结果。如果中心点选择不当，分类的结果就可能不是很理想。</p></li><li><p>常用的方法有以下：</p><ul><li>随机选取 K 个样本点作为初始的类群中心点。</li><li>先选择第 1 个样本点当第 1 群的中心；其次选择与第 1 个中心点的距离超过既定标准的下一个样本点当第 2 群的中心点；接着选择与第 1、2 个中心点的距离超过既定标准的下一个样本当第 3 群的中心；依此类推，直到选出 K 个群的中心为止。</li></ul></li></ul><h2 id="alglgorithm-pam" tabindex="-1"><a class="header-anchor" href="#alglgorithm-pam" aria-hidden="true">#</a> Alglgorithm PAM</h2><figure><img src="'+O+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+B+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>K-Means can handle large data sets efficiently but is limited to interval-scaled attributes and sensitive to outliers. PAM is capable of handling various attribute types but is not efficient when clustering large data sets.</p><h2 id="som-example-initialize-prototype" tabindex="-1"><a class="header-anchor" href="#som-example-initialize-prototype" aria-hidden="true">#</a> SOM example: initialize prototype</h2><p><img src="'+N+'" alt="text" loading="lazy"><img src="'+I+'" alt="text" loading="lazy"><img src="'+R+'" alt="text" loading="lazy"><img src="'+j+'" alt="text" loading="lazy"><img src="'+P+'" alt="text" loading="lazy"><img src="'+q+'" alt="text" loading="lazy"></p><h2 id="模糊聚类" tabindex="-1"><a class="header-anchor" href="#模糊聚类" aria-hidden="true">#</a> 模糊聚类</h2><ul><li>Hard (Exclusive) Clustering. <ul><li>Objects are assigned to a single cluster. K-Means.</li></ul></li><li>Soft (Non-Exclusive) Clustering. <ul><li>Objects have probability distribution over clusters.EM (Soft Version of K-Means).</li></ul></li></ul><h2 id="k-means-a-special-kind-of-em" tabindex="-1"><a class="header-anchor" href="#k-means-a-special-kind-of-em" aria-hidden="true">#</a> K-Means(A Special Kind of EM)</h2><ul><li>Initialization. Randomly select cluster centers.</li><li>E Step. <ul><li>Euclidean Distance to Each Cluster Center</li><li>The probability is either 0 or 1.</li></ul></li><li>M Step. Recalculate cluster centers.</li><li>Convergence. Cluster centers do not change.</li></ul><h2 id="em" tabindex="-1"><a class="header-anchor" href="#em" aria-hidden="true">#</a> EM</h2><ul><li>Initialization. Randomly select cluster centers and variances.</li><li>E Step. <ul><li>Gaussian Distribution for Each Cluster.</li><li>The probability is between 0 and 1.</li></ul></li><li>M Step. Recalculate cluster centers and variances.</li><li>Convergence. <ul><li>Cluster centers do not change.</li></ul></li></ul><figure><img src="'+T+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="密度聚类算法-dbscan" tabindex="-1"><a class="header-anchor" href="#密度聚类算法-dbscan" aria-hidden="true">#</a> 密度聚类算法(DBSCAN)</h2><ul><li><p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is proposed in 1996.</p></li><li><p>Unlike the K-mean, DBSCAN does not need to specify the number of clusters.</p><ul><li>It can automatically detect the number of clusters based on your input data and parameters.</li></ul></li><li><p>DBSCAN can find arbitrary shape clusters that k-means are not able to find.</p></li><li><p>DBSCAN can handle noise and outliers.</p></li><li><p>All the outliers will be identified and marked without been classified into any cluster.</p></li><li><p>Therefore, DBSCAN can also be used for Anomaly Detection (Outlier Detection).</p></li><li><p>There are two parameters we need to set for DBSCAN.</p><ul><li>Eps: Maximum radius of the neighborhood.</li><li>MinPts: Minimum number of points in an Eps-neighbourhood of that point.</li></ul></li></ul><figure><img src="'+F+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p><img src="'+L+'" alt="text" loading="lazy"><img src="'+H+'" alt="text" loading="lazy"><img src="'+W+'" alt="text" loading="lazy"><img src="'+V+'" alt="text" loading="lazy"><img src="'+G+'" alt="text" loading="lazy"></p><p>优点：</p><ul><li>不需要事先指定群集数。</li><li>可轻松处理噪音，不受离群值影响。</li><li>它没有严格的形状，它可以正确地容纳许多数据点。</li></ul><p>缺点：</p><ul><li>无法使用不同密度的数据集。</li><li>对聚类超参数敏感：eps 和 min_points。</li><li>如果数据过于稀疏，则表现不佳。</li><li>密度测量（密度可达和密度相连）可能受抽样影响。</li></ul><h2 id="群数的判断-1" tabindex="-1"><a class="header-anchor" href="#群数的判断-1" aria-hidden="true">#</a> 群数的判断</h2><p>有四种常用的群数判断方法：</p><ul><li>Sum of Square (SS)。</li><li>R-Squared (R2)。</li><li>Semi-Partial R-Squared。</li><li>Silhouette Coefficient (轮廓系数)。</li></ul><h2 id="群数的判断-r-2" tabindex="-1"><a class="header-anchor" href="#群数的判断-r-2" aria-hidden="true">#</a> 群数的判断(R^2)</h2><ol><li>SS 可分为组间（SSB）、组内（SSW）及全体（SST），SSB+SSW=SST。</li><li>随着观察体（集群）的合并，集群数目愈来愈少，集群内的异质性愈来愈高，因此 SSW 就会愈来愈大。</li><li>由于聚类分析的主要目的在使集群内的差异较小，而集群间的差异较大，因此 R²要大一些。</li><li>如果由于观察体（集群）的合并，使得 R²突然减小，表示应停止合并。</li></ol><figure><img src="'+Y+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="群数的判断-semi-partial-r2" tabindex="-1"><a class="header-anchor" href="#群数的判断-semi-partial-r2" aria-hidden="true">#</a> 群数的判断(Semi-Partial R2)</h2><figure><img src="'+Z+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="轮廓系数-silhouette-coefficient" tabindex="-1"><a class="header-anchor" href="#轮廓系数-silhouette-coefficient" aria-hidden="true">#</a> 轮廓系数(Silhouette Coefficient)</h2><p><img src="'+U+'" alt="alt text" loading="lazy"><img src="'+J+'" alt="alt text" loading="lazy"><img src="'+X+'" alt="text" loading="lazy"><img src="'+Q+'" alt="text" loading="lazy"><img src="'+$+'" alt="text" loading="lazy"><img src="'+ii+'" alt="text" loading="lazy"><img src="'+ti+'" alt="text" loading="lazy"><img src="'+ei+'" alt="text" loading="lazy"><img src="'+ai+'" alt="text" loading="lazy"></p>',78),ni=[si];function ri(oi,ci){return t(),e("div",null,ni)}const pi=i(li,[["render",ri],["__file","4_6聚类分析.html.vue"]]);export{pi as default};
