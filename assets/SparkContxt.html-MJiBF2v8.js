const t=JSON.parse('{"key":"v-e83859d8","path":"/8%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B/3spark/%E5%9F%BA%E7%A1%80/SparkContxt.html","title":"SparkContxt","lang":"zh-CN","frontmatter":{"description":"SparkContxt Spark RDD 编程的程序入口对象是SparkContext对象(不论何种编程语言),本质上, SparkContext对编程来说, 主要功能就是创建第一个RDD出来 from pyspark import SparkConf, SparkContext conf = SparkConf().setAppName(\\"test\\").setMaster(\\"local[*]\\") sc = SparkContext(conf=conf)","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/8%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B/3spark/%E5%9F%BA%E7%A1%80/SparkContxt.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"SparkContxt"}],["meta",{"property":"og:description","content":"SparkContxt Spark RDD 编程的程序入口对象是SparkContext对象(不论何种编程语言),本质上, SparkContext对编程来说, 主要功能就是创建第一个RDD出来 from pyspark import SparkConf, SparkContext conf = SparkConf().setAppName(\\"test\\").setMaster(\\"local[*]\\") sc = SparkContext(conf=conf)"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-24T12:46:58.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-24T12:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"SparkContxt\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-24T12:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[],"git":{"createdTime":1740401218000,"updatedTime":1740401218000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":0.21,"words":64},"filePathRelative":"8数据工程/3spark/基础/SparkContxt.md","localizedDate":"2025年2月24日","excerpt":"<h1> SparkContxt</h1>\\n<p>Spark RDD 编程的程序入口对象是SparkContext对象(不论何种编程语言),本质上, SparkContext对编程来说, 主要功能就是创建第一个RDD出来</p>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">from</span> pyspark <span class=\\"token keyword\\">import</span> SparkConf<span class=\\"token punctuation\\">,</span> SparkContext\\nconf <span class=\\"token operator\\">=</span> SparkConf<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>setAppName<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>setMaster<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"local[*]\\"</span><span class=\\"token punctuation\\">)</span>\\nsc <span class=\\"token operator\\">=</span> SparkContext<span class=\\"token punctuation\\">(</span>conf<span class=\\"token operator\\">=</span>conf<span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}');export{t as data};
