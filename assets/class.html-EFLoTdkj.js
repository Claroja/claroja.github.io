const n=JSON.parse(`{"key":"v-0101ab21","path":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3_2pytorch/%E6%9C%AA%E5%88%86%E7%B1%BB/class.html","title":"class","lang":"zh-CN","frontmatter":{"description":"class import torch from transformers import AutoTokenizer from transformers import AutoModelForSequenceClassification from datasets import Dataset, ClassLabel, load_metric from torch.utils.data import DataLoader from torch.optim import AdamW from transformers import get_scheduler import pandas as pd from pathlib import Path def load_device(device_name): \\"\\"\\" use GPU or CPU? Args: device_name (str): 'auto' means that will use gpu, if there is. Returns: torch.device \\"\\"\\" if device_name == 'auto': return torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") else: return device_name def load_tokenizer(model_path): \\"\\"\\" load_tokenizer Args: model_path (str): full model path Returns: transformers.tokenizer \\"\\"\\" return AutoTokenizer.from_pretrained(model_path) def load_model(model_path,device,num_labels): \\"\\"\\" load model Args: model_path (str): full model path device (str): model's device num_labels (int): the nums of output Returns: transformers.model \\"\\"\\" return AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=num_labels).to(device) class BertTrain(object): def __init__(self, model_name,task_name,labels,expand=False,train_or_predict=\\"predict\\",test_size=0.2,device_name='auto' ,num_epochs = 5,batch_size = 8): \\"\\"\\" init instance Args: model_name (str): only model name, exclude path. task_name (str): is this model for what? num_labels (int): nums of output labels (list): lables of output, the index is the num of label train_or_predict (str): is this for training model or using model for predicting. if train, the model_path will be \\"./pretrained_model/\\" + model_name, the path to load pretrained model. and new_model_path = \\"./finetuning_model/\\" + task_name, the path to save trained model. if predict, the mode_path will be \\"./finetuning_model/\\" + task_name, the path to load finetuning model. test_size (str): if training model, the percentage of train samples of all device_name (str): cpu or gpu num_epochs (int): how many epochs batch_size (int): size of batch \\"\\"\\" self.task_name = task_name self.model_name = model_name self.dir_path = \\"./finetuning_model/\\" if train_or_predict == \\"predict\\" else \\"./pretrained_model/\\" self.model_path = self.dir_path + task_name if train_or_predict == \\"predict\\" else self.dir_path + model_name self.device = load_device(device_name) self.tokenizer = load_tokenizer(self.model_path) self.model = load_model(self.model_path, self.device, len(labels)) self.classLabel = ClassLabel(names=labels) self.expand = expand if train_or_predict == \\"predict\\": # delete method irrelevant to predict mod pass ## delattr(__class__,\\"data_process\\") ## delattr(__class__, \\"train\\") ## delattr(__class__, \\"one_stop_train\\") else: # add attribute relevant train mod self.num_epochs=num_epochs self.batch_size=batch_size self.test_size = test_size self.new_model_path = \\"./pre_finetuning_model/\\" + task_name def data_process(self,df): \\"\\"\\" preprocess data. 1. split dataframe into train and test by test_size and stratify by \\"labels\\" 2. tokenize \\"text\\" 3. return [train_dataloader,eval_dataloader,df] Args: df (pandas.DataFrame): the df mast have columns named \\"text\\" and \\"labels\\" Returns: [train_dataloader,eval_dataloader,df] \\"\\"\\" text_column_name = 'text' labels_column_name = 'labels' assert text_column_name in df.columns assert labels_column_name in df.columns df_with_index = df.reset_index() df_for_train = df_with_index.loc[:, [\\"index\\",text_column_name, labels_column_name]] dataset = Dataset.from_pandas(df_for_train) dataset = dataset.cast_column(labels_column_name, self.classLabel) datadict = dataset.train_test_split(test_size=self.test_size, stratify_by_column= labels_column_name,seed=10) test_df = datadict[\\"test\\"].to_pandas() train_df = datadict[\\"train\\"].to_pandas() print(\\"hello1\\") # expand if self.expand: label_id = self.classLabel.str2int(\\"youchoice\\") print(label_id) df_expand = pd.concat([*([train_df[train_df[\\"labels\\"] == label_id]] *10)]) print(df_expand) del df_expand[\\"index\\"] dict_expand = df_expand.to_dict(orient=\\"records\\") for item in dict_expand: datadict[\\"train\\"] = datadict[\\"train\\"].add_item(item) print(datadict) # expand df_with_index.loc[df_with_index[\\"index\\"].isin(test_df[\\"index\\"]),\\"splitmark\\"] = \\"test\\" df_with_index.loc[df_with_index[\\"index\\"].isin(train_df[\\"index\\"]), \\"splitmark\\"] = \\"train\\" df_with_splitmark = df_with_index.set_index(\\"index\\") tokenized_datadict = datadict.map(lambda x:self.tokenizer(x[text_column_name], padding='max_length', truncation=True, max_length=512,), batched=True,remove_columns = [\\"index\\",text_column_name]) tokenized_datadict.set_format(\\"torch\\") train_dataloader = DataLoader(tokenized_datadict[\\"train\\"], shuffle=True, batch_size=4) eval_dataloader = DataLoader(tokenized_datadict[\\"test\\"], batch_size=4) return train_dataloader,eval_dataloader,df_with_splitmark def train(self,train_dataloader,eval_dataloader,df_with_splitmark): \\"\\"\\" train the model Args: train_dataloader: eval_dataloader: Returns: train the self.model \\"\\"\\" optimizer = AdamW(self.model.parameters(), lr=5e-5) num_training_steps = self.num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( name=\\"linear\\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps ) for epoch in range(self.num_epochs): print(epoch) self.model.train() for batch in train_dataloader: batch = {k: v.to(self.device) for k, v in batch.items()} outputs = self.model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() self.model.eval() for batch in eval_dataloader: batch = {k: v.to(self.device) for k, v in batch.items()} with torch.no_grad(): outputs = self.model(**batch) # -1 would thus map to the last dimension, -2 to the preceding one, etc. logits = outputs.logits predictions = torch.argmax(logits, dim=-1) print(epoch, loss) eval_df = df_with_splitmark.loc[df_with_splitmark[\\"splitmark\\"]==\\"test\\"].copy() for i in eval_df.index: result = self.predict(eval_df.loc[i]['text']) eval_df.loc[i,'predict_num']=result[\\"num\\"] eval_df.loc[i, 'predict_labels'] = result[\\"labels\\"] confusion_df=self.evaluate(eval_df[\\"labels\\"],eval_df[\\"predict_labels\\"]) self.save_all(df_with_splitmark,eval_df,confusion_df,epoch) def one_stop_train(self,df): \\"\\"\\" train, evaluate and save model&amp;tokenizer&amp;data Args: df: original dataFrame Returns: none \\"\\"\\" [train_dataloader,eval_dataloader,df_with_splitmark] = self.data_process(df) self.train(train_dataloader,eval_dataloader,df_with_splitmark) def predict(self,text): \\"\\"\\" predict using model Args: text (str): one sentence Returns (dict): \\"num\\": lable mapped to num \\"labels\\": num mapped to label \\"\\"\\" self.model.eval() data = self.tokenizer(text, truncation=True,max_length=512) data = {k: torch.tensor([v]).to(self.device) for k, v in data.items()} with torch.no_grad(): outputs = self.model(**data) result = torch.argmax(outputs.logits, dim=-1).to(\\"cpu\\").numpy() result = int(result[0]) return {\\"num\\":result,\\"labels\\":self.classLabel.int2str(result)} def evaluate(self,labels_series,predictLabels_series): \\"\\"\\" create confusion matrix. Args: labels_series (pandas.Series): marked labels predictLabel_series (pandas.Series): predict labels Returns: confusion_df (pandas.DataFrame): confusion df that inclues all class recall and precision \\"\\"\\" confusion_df = pd.crosstab(labels_series,predictLabels_series, dropna=False) cha = set(confusion_df.columns) ^ set(confusion_df.index) if len(cha)&gt;0: if(len(confusion_df.index)&gt;len(confusion_df.columns)): confusion_df.loc[:,cha] =0 else: confusion_df.loc[cha,:]=0 confusion_df = confusion_df.reindex(index=self.classLabel.names,columns=self.classLabel.names) precision = confusion_df.apply(lambda x: x[x.name] / x.sum() if x.sum()!=0 else 0) recall = confusion_df.apply(lambda x: x[x.name] / x.sum() if x.sum()!=0 else 0,axis=1) confusion_df.loc[\\"precision\\"]= precision confusion_df.loc[:,\\"recall\\"]=recall return confusion_df def save_all(self,df_with_splitmark,eval_df,confusion_df,epoch): new_model_path_with_epoch = f\\"{self.new_model_path}_{epoch}/\\" path = Path(f\\"./{new_model_path_with_epoch}\\") path.mkdir() self.model.save_pretrained(new_model_path_with_epoch) self.tokenizer.save_pretrained(new_model_path_with_epoch) df_with_splitmark.to_csv(f\\"{new_model_path_with_epoch}df_with_splitmark.csv\\") eval_df.to_csv(f\\"{new_model_path_with_epoch}eval_df.csv\\") confusion_df.to_csv(f\\"{new_model_path_with_epoch}confusion_df.csv\\")","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3_2pytorch/%E6%9C%AA%E5%88%86%E7%B1%BB/class.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"class"}],["meta",{"property":"og:description","content":"class import torch from transformers import AutoTokenizer from transformers import AutoModelForSequenceClassification from datasets import Dataset, ClassLabel, load_metric from torch.utils.data import DataLoader from torch.optim import AdamW from transformers import get_scheduler import pandas as pd from pathlib import Path def load_device(device_name): \\"\\"\\" use GPU or CPU? Args: device_name (str): 'auto' means that will use gpu, if there is. Returns: torch.device \\"\\"\\" if device_name == 'auto': return torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") else: return device_name def load_tokenizer(model_path): \\"\\"\\" load_tokenizer Args: model_path (str): full model path Returns: transformers.tokenizer \\"\\"\\" return AutoTokenizer.from_pretrained(model_path) def load_model(model_path,device,num_labels): \\"\\"\\" load model Args: model_path (str): full model path device (str): model's device num_labels (int): the nums of output Returns: transformers.model \\"\\"\\" return AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=num_labels).to(device) class BertTrain(object): def __init__(self, model_name,task_name,labels,expand=False,train_or_predict=\\"predict\\",test_size=0.2,device_name='auto' ,num_epochs = 5,batch_size = 8): \\"\\"\\" init instance Args: model_name (str): only model name, exclude path. task_name (str): is this model for what? num_labels (int): nums of output labels (list): lables of output, the index is the num of label train_or_predict (str): is this for training model or using model for predicting. if train, the model_path will be \\"./pretrained_model/\\" + model_name, the path to load pretrained model. and new_model_path = \\"./finetuning_model/\\" + task_name, the path to save trained model. if predict, the mode_path will be \\"./finetuning_model/\\" + task_name, the path to load finetuning model. test_size (str): if training model, the percentage of train samples of all device_name (str): cpu or gpu num_epochs (int): how many epochs batch_size (int): size of batch \\"\\"\\" self.task_name = task_name self.model_name = model_name self.dir_path = \\"./finetuning_model/\\" if train_or_predict == \\"predict\\" else \\"./pretrained_model/\\" self.model_path = self.dir_path + task_name if train_or_predict == \\"predict\\" else self.dir_path + model_name self.device = load_device(device_name) self.tokenizer = load_tokenizer(self.model_path) self.model = load_model(self.model_path, self.device, len(labels)) self.classLabel = ClassLabel(names=labels) self.expand = expand if train_or_predict == \\"predict\\": # delete method irrelevant to predict mod pass ## delattr(__class__,\\"data_process\\") ## delattr(__class__, \\"train\\") ## delattr(__class__, \\"one_stop_train\\") else: # add attribute relevant train mod self.num_epochs=num_epochs self.batch_size=batch_size self.test_size = test_size self.new_model_path = \\"./pre_finetuning_model/\\" + task_name def data_process(self,df): \\"\\"\\" preprocess data. 1. split dataframe into train and test by test_size and stratify by \\"labels\\" 2. tokenize \\"text\\" 3. return [train_dataloader,eval_dataloader,df] Args: df (pandas.DataFrame): the df mast have columns named \\"text\\" and \\"labels\\" Returns: [train_dataloader,eval_dataloader,df] \\"\\"\\" text_column_name = 'text' labels_column_name = 'labels' assert text_column_name in df.columns assert labels_column_name in df.columns df_with_index = df.reset_index() df_for_train = df_with_index.loc[:, [\\"index\\",text_column_name, labels_column_name]] dataset = Dataset.from_pandas(df_for_train) dataset = dataset.cast_column(labels_column_name, self.classLabel) datadict = dataset.train_test_split(test_size=self.test_size, stratify_by_column= labels_column_name,seed=10) test_df = datadict[\\"test\\"].to_pandas() train_df = datadict[\\"train\\"].to_pandas() print(\\"hello1\\") # expand if self.expand: label_id = self.classLabel.str2int(\\"youchoice\\") print(label_id) df_expand = pd.concat([*([train_df[train_df[\\"labels\\"] == label_id]] *10)]) print(df_expand) del df_expand[\\"index\\"] dict_expand = df_expand.to_dict(orient=\\"records\\") for item in dict_expand: datadict[\\"train\\"] = datadict[\\"train\\"].add_item(item) print(datadict) # expand df_with_index.loc[df_with_index[\\"index\\"].isin(test_df[\\"index\\"]),\\"splitmark\\"] = \\"test\\" df_with_index.loc[df_with_index[\\"index\\"].isin(train_df[\\"index\\"]), \\"splitmark\\"] = \\"train\\" df_with_splitmark = df_with_index.set_index(\\"index\\") tokenized_datadict = datadict.map(lambda x:self.tokenizer(x[text_column_name], padding='max_length', truncation=True, max_length=512,), batched=True,remove_columns = [\\"index\\",text_column_name]) tokenized_datadict.set_format(\\"torch\\") train_dataloader = DataLoader(tokenized_datadict[\\"train\\"], shuffle=True, batch_size=4) eval_dataloader = DataLoader(tokenized_datadict[\\"test\\"], batch_size=4) return train_dataloader,eval_dataloader,df_with_splitmark def train(self,train_dataloader,eval_dataloader,df_with_splitmark): \\"\\"\\" train the model Args: train_dataloader: eval_dataloader: Returns: train the self.model \\"\\"\\" optimizer = AdamW(self.model.parameters(), lr=5e-5) num_training_steps = self.num_epochs * len(train_dataloader) lr_scheduler = get_scheduler( name=\\"linear\\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps ) for epoch in range(self.num_epochs): print(epoch) self.model.train() for batch in train_dataloader: batch = {k: v.to(self.device) for k, v in batch.items()} outputs = self.model(**batch) loss = outputs.loss loss.backward() optimizer.step() lr_scheduler.step() optimizer.zero_grad() self.model.eval() for batch in eval_dataloader: batch = {k: v.to(self.device) for k, v in batch.items()} with torch.no_grad(): outputs = self.model(**batch) # -1 would thus map to the last dimension, -2 to the preceding one, etc. logits = outputs.logits predictions = torch.argmax(logits, dim=-1) print(epoch, loss) eval_df = df_with_splitmark.loc[df_with_splitmark[\\"splitmark\\"]==\\"test\\"].copy() for i in eval_df.index: result = self.predict(eval_df.loc[i]['text']) eval_df.loc[i,'predict_num']=result[\\"num\\"] eval_df.loc[i, 'predict_labels'] = result[\\"labels\\"] confusion_df=self.evaluate(eval_df[\\"labels\\"],eval_df[\\"predict_labels\\"]) self.save_all(df_with_splitmark,eval_df,confusion_df,epoch) def one_stop_train(self,df): \\"\\"\\" train, evaluate and save model&amp;tokenizer&amp;data Args: df: original dataFrame Returns: none \\"\\"\\" [train_dataloader,eval_dataloader,df_with_splitmark] = self.data_process(df) self.train(train_dataloader,eval_dataloader,df_with_splitmark) def predict(self,text): \\"\\"\\" predict using model Args: text (str): one sentence Returns (dict): \\"num\\": lable mapped to num \\"labels\\": num mapped to label \\"\\"\\" self.model.eval() data = self.tokenizer(text, truncation=True,max_length=512) data = {k: torch.tensor([v]).to(self.device) for k, v in data.items()} with torch.no_grad(): outputs = self.model(**data) result = torch.argmax(outputs.logits, dim=-1).to(\\"cpu\\").numpy() result = int(result[0]) return {\\"num\\":result,\\"labels\\":self.classLabel.int2str(result)} def evaluate(self,labels_series,predictLabels_series): \\"\\"\\" create confusion matrix. Args: labels_series (pandas.Series): marked labels predictLabel_series (pandas.Series): predict labels Returns: confusion_df (pandas.DataFrame): confusion df that inclues all class recall and precision \\"\\"\\" confusion_df = pd.crosstab(labels_series,predictLabels_series, dropna=False) cha = set(confusion_df.columns) ^ set(confusion_df.index) if len(cha)&gt;0: if(len(confusion_df.index)&gt;len(confusion_df.columns)): confusion_df.loc[:,cha] =0 else: confusion_df.loc[cha,:]=0 confusion_df = confusion_df.reindex(index=self.classLabel.names,columns=self.classLabel.names) precision = confusion_df.apply(lambda x: x[x.name] / x.sum() if x.sum()!=0 else 0) recall = confusion_df.apply(lambda x: x[x.name] / x.sum() if x.sum()!=0 else 0,axis=1) confusion_df.loc[\\"precision\\"]= precision confusion_df.loc[:,\\"recall\\"]=recall return confusion_df def save_all(self,df_with_splitmark,eval_df,confusion_df,epoch): new_model_path_with_epoch = f\\"{self.new_model_path}_{epoch}/\\" path = Path(f\\"./{new_model_path_with_epoch}\\") path.mkdir() self.model.save_pretrained(new_model_path_with_epoch) self.tokenizer.save_pretrained(new_model_path_with_epoch) df_with_splitmark.to_csv(f\\"{new_model_path_with_epoch}df_with_splitmark.csv\\") eval_df.to_csv(f\\"{new_model_path_with_epoch}eval_df.csv\\") confusion_df.to_csv(f\\"{new_model_path_with_epoch}confusion_df.csv\\")"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-18T14:02:01.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-18T14:02:01.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"class\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-18T14:02:01.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[],"git":{"createdTime":1739887321000,"updatedTime":1739887321000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":2.92,"words":875},"filePathRelative":"机器学习/3_2pytorch/未分类/class.md","localizedDate":"2025年2月18日","excerpt":"<h1> class</h1>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> torch\\n<span class=\\"token keyword\\">from</span> transformers <span class=\\"token keyword\\">import</span> AutoTokenizer\\n<span class=\\"token keyword\\">from</span> transformers <span class=\\"token keyword\\">import</span> AutoModelForSequenceClassification\\n<span class=\\"token keyword\\">from</span> datasets <span class=\\"token keyword\\">import</span> Dataset<span class=\\"token punctuation\\">,</span> ClassLabel<span class=\\"token punctuation\\">,</span> load_metric\\n<span class=\\"token keyword\\">from</span> torch<span class=\\"token punctuation\\">.</span>utils<span class=\\"token punctuation\\">.</span>data <span class=\\"token keyword\\">import</span> DataLoader\\n<span class=\\"token keyword\\">from</span> torch<span class=\\"token punctuation\\">.</span>optim <span class=\\"token keyword\\">import</span> AdamW\\n<span class=\\"token keyword\\">from</span> transformers <span class=\\"token keyword\\">import</span> get_scheduler\\n<span class=\\"token keyword\\">import</span> pandas <span class=\\"token keyword\\">as</span> pd\\n<span class=\\"token keyword\\">from</span> pathlib <span class=\\"token keyword\\">import</span> Path\\n\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">load_device</span><span class=\\"token punctuation\\">(</span>device_name<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n    use GPU or CPU?\\n\\n    Args:\\n        device_name (str): 'auto' means that will use gpu, if there is.\\n    Returns:\\n        torch.device\\n\\n    \\"\\"\\"</span>\\n    <span class=\\"token keyword\\">if</span> device_name <span class=\\"token operator\\">==</span> <span class=\\"token string\\">'auto'</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token keyword\\">return</span> torch<span class=\\"token punctuation\\">.</span>device<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"cuda\\"</span> <span class=\\"token keyword\\">if</span> torch<span class=\\"token punctuation\\">.</span>cuda<span class=\\"token punctuation\\">.</span>is_available<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">else</span> <span class=\\"token string\\">\\"cpu\\"</span><span class=\\"token punctuation\\">)</span>\\n\\n    <span class=\\"token keyword\\">else</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token keyword\\">return</span> device_name\\n\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">load_tokenizer</span><span class=\\"token punctuation\\">(</span>model_path<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n    load_tokenizer\\n\\n    Args:\\n        model_path (str): full model path\\n\\n    Returns:\\n        transformers.tokenizer\\n\\n    \\"\\"\\"</span>\\n    <span class=\\"token keyword\\">return</span> AutoTokenizer<span class=\\"token punctuation\\">.</span>from_pretrained<span class=\\"token punctuation\\">(</span>model_path<span class=\\"token punctuation\\">)</span>\\n\\n\\n<span class=\\"token keyword\\">def</span> <span class=\\"token function\\">load_model</span><span class=\\"token punctuation\\">(</span>model_path<span class=\\"token punctuation\\">,</span>device<span class=\\"token punctuation\\">,</span>num_labels<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n    load model\\n\\n    Args:\\n        model_path (str): full model path\\n        device (str): model's device\\n        num_labels (int): the nums of output\\n\\n    Returns:\\n        transformers.model\\n\\n    \\"\\"\\"</span>\\n    <span class=\\"token keyword\\">return</span> AutoModelForSequenceClassification<span class=\\"token punctuation\\">.</span>from_pretrained<span class=\\"token punctuation\\">(</span>model_path<span class=\\"token punctuation\\">,</span>num_labels<span class=\\"token operator\\">=</span>num_labels<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>to<span class=\\"token punctuation\\">(</span>device<span class=\\"token punctuation\\">)</span>\\n\\n\\n<span class=\\"token keyword\\">class</span> <span class=\\"token class-name\\">BertTrain</span><span class=\\"token punctuation\\">(</span><span class=\\"token builtin\\">object</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">__init__</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span> model_name<span class=\\"token punctuation\\">,</span>task_name<span class=\\"token punctuation\\">,</span>labels<span class=\\"token punctuation\\">,</span>expand<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">False</span><span class=\\"token punctuation\\">,</span>train_or_predict<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"predict\\"</span><span class=\\"token punctuation\\">,</span>test_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.2</span><span class=\\"token punctuation\\">,</span>device_name<span class=\\"token operator\\">=</span><span class=\\"token string\\">'auto'</span>\\n                <span class=\\"token punctuation\\">,</span>num_epochs <span class=\\"token operator\\">=</span> <span class=\\"token number\\">5</span><span class=\\"token punctuation\\">,</span>batch_size <span class=\\"token operator\\">=</span> <span class=\\"token number\\">8</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        init instance\\n\\n        Args:\\n            model_name (str): only model name, exclude path.\\n            task_name (str): is this model for what?\\n            num_labels (int): nums of output\\n            labels (list): lables of output, the index is the num of label\\n            train_or_predict (str): is this for training model or using model for predicting.\\n                if train, the model_path will be \\"./pretrained_model/\\" + model_name, the path to load pretrained model.\\n                    and new_model_path = \\"./finetuning_model/\\" + task_name, the path to save trained model.\\n                if predict, the mode_path will be \\"./finetuning_model/\\" + task_name, the path to load finetuning model.\\n            test_size (str): if training model, the percentage of train samples of all\\n            device_name (str): cpu or gpu\\n            num_epochs (int): how many epochs\\n            batch_size (int): size of batch\\n        \\"\\"\\"</span>\\n        self<span class=\\"token punctuation\\">.</span>task_name <span class=\\"token operator\\">=</span> task_name\\n        self<span class=\\"token punctuation\\">.</span>model_name <span class=\\"token operator\\">=</span> model_name\\n        self<span class=\\"token punctuation\\">.</span>dir_path <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"./finetuning_model/\\"</span> <span class=\\"token keyword\\">if</span> train_or_predict <span class=\\"token operator\\">==</span> <span class=\\"token string\\">\\"predict\\"</span> <span class=\\"token keyword\\">else</span> <span class=\\"token string\\">\\"./pretrained_model/\\"</span>\\n        self<span class=\\"token punctuation\\">.</span>model_path <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>dir_path <span class=\\"token operator\\">+</span> task_name <span class=\\"token keyword\\">if</span> train_or_predict <span class=\\"token operator\\">==</span> <span class=\\"token string\\">\\"predict\\"</span> <span class=\\"token keyword\\">else</span> self<span class=\\"token punctuation\\">.</span>dir_path <span class=\\"token operator\\">+</span> model_name\\n        self<span class=\\"token punctuation\\">.</span>device <span class=\\"token operator\\">=</span> load_device<span class=\\"token punctuation\\">(</span>device_name<span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>tokenizer <span class=\\"token operator\\">=</span> load_tokenizer<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>model_path<span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>model <span class=\\"token operator\\">=</span> load_model<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>model_path<span class=\\"token punctuation\\">,</span> self<span class=\\"token punctuation\\">.</span>device<span class=\\"token punctuation\\">,</span> <span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>labels<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>classLabel <span class=\\"token operator\\">=</span> ClassLabel<span class=\\"token punctuation\\">(</span>names<span class=\\"token operator\\">=</span>labels<span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>expand <span class=\\"token operator\\">=</span> expand\\n\\n        <span class=\\"token keyword\\">if</span> train_or_predict <span class=\\"token operator\\">==</span> <span class=\\"token string\\">\\"predict\\"</span><span class=\\"token punctuation\\">:</span>  <span class=\\"token comment\\"># delete method irrelevant to predict mod</span>\\n            <span class=\\"token keyword\\">pass</span>\\n<span class=\\"token comment\\">##             delattr(__class__,\\"data_process\\")</span>\\n<span class=\\"token comment\\">##             delattr(__class__, \\"train\\")</span>\\n<span class=\\"token comment\\">##             delattr(__class__, \\"one_stop_train\\")</span>\\n        <span class=\\"token keyword\\">else</span><span class=\\"token punctuation\\">:</span>  <span class=\\"token comment\\"># add attribute relevant train mod</span>\\n            self<span class=\\"token punctuation\\">.</span>num_epochs<span class=\\"token operator\\">=</span>num_epochs\\n            self<span class=\\"token punctuation\\">.</span>batch_size<span class=\\"token operator\\">=</span>batch_size\\n            self<span class=\\"token punctuation\\">.</span>test_size <span class=\\"token operator\\">=</span> test_size\\n            self<span class=\\"token punctuation\\">.</span>new_model_path <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"./pre_finetuning_model/\\"</span> <span class=\\"token operator\\">+</span> task_name\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">data_process</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>df<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        preprocess data.\\n            1. split dataframe into train and test by test_size and stratify by \\"labels\\"\\n            2. tokenize \\"text\\"\\n            3. return [train_dataloader,eval_dataloader,df]\\n\\n        Args:\\n            df (pandas.DataFrame): the df mast have columns named \\"text\\" and \\"labels\\"\\n\\n        Returns:\\n            [train_dataloader,eval_dataloader,df]\\n\\n        \\"\\"\\"</span>\\n        text_column_name <span class=\\"token operator\\">=</span> <span class=\\"token string\\">'text'</span>\\n        labels_column_name <span class=\\"token operator\\">=</span> <span class=\\"token string\\">'labels'</span>\\n        <span class=\\"token keyword\\">assert</span> text_column_name <span class=\\"token keyword\\">in</span> df<span class=\\"token punctuation\\">.</span>columns\\n        <span class=\\"token keyword\\">assert</span> labels_column_name <span class=\\"token keyword\\">in</span> df<span class=\\"token punctuation\\">.</span>columns\\n\\n        df_with_index <span class=\\"token operator\\">=</span> df<span class=\\"token punctuation\\">.</span>reset_index<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        df_for_train <span class=\\"token operator\\">=</span> df_with_index<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">,</span> <span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">,</span>text_column_name<span class=\\"token punctuation\\">,</span> labels_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">]</span>\\n\\n        dataset <span class=\\"token operator\\">=</span> Dataset<span class=\\"token punctuation\\">.</span>from_pandas<span class=\\"token punctuation\\">(</span>df_for_train<span class=\\"token punctuation\\">)</span>\\n        dataset <span class=\\"token operator\\">=</span> dataset<span class=\\"token punctuation\\">.</span>cast_column<span class=\\"token punctuation\\">(</span>labels_column_name<span class=\\"token punctuation\\">,</span> self<span class=\\"token punctuation\\">.</span>classLabel<span class=\\"token punctuation\\">)</span>\\n        datadict <span class=\\"token operator\\">=</span> dataset<span class=\\"token punctuation\\">.</span>train_test_split<span class=\\"token punctuation\\">(</span>test_size<span class=\\"token operator\\">=</span>self<span class=\\"token punctuation\\">.</span>test_size<span class=\\"token punctuation\\">,</span> stratify_by_column<span class=\\"token operator\\">=</span> labels_column_name<span class=\\"token punctuation\\">,</span>seed<span class=\\"token operator\\">=</span><span class=\\"token number\\">10</span><span class=\\"token punctuation\\">)</span>\\n        \\n        test_df <span class=\\"token operator\\">=</span> datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>to_pandas<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        train_df <span class=\\"token operator\\">=</span> datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>to_pandas<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"hello1\\"</span><span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token comment\\"># expand</span>\\n        <span class=\\"token keyword\\">if</span> self<span class=\\"token punctuation\\">.</span>expand<span class=\\"token punctuation\\">:</span>\\n            label_id <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>classLabel<span class=\\"token punctuation\\">.</span>str2int<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"youchoice\\"</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>label_id<span class=\\"token punctuation\\">)</span>\\n            df_expand <span class=\\"token operator\\">=</span> pd<span class=\\"token punctuation\\">.</span>concat<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">[</span><span class=\\"token operator\\">*</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">[</span>train_df<span class=\\"token punctuation\\">[</span>train_df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">==</span> label_id<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">*</span><span class=\\"token number\\">10</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>df_expand<span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">del</span> df_expand<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span>\\n            dict_expand <span class=\\"token operator\\">=</span> df_expand<span class=\\"token punctuation\\">.</span>to_dict<span class=\\"token punctuation\\">(</span>orient<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"records\\"</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">for</span> item <span class=\\"token keyword\\">in</span> dict_expand<span class=\\"token punctuation\\">:</span>\\n                datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>add_item<span class=\\"token punctuation\\">(</span>item<span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>datadict<span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token comment\\"># expand</span>\\n\\n        df_with_index<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>isin<span class=\\"token punctuation\\">(</span>test_df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"test\\"</span>\\n        df_with_index<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_index<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>isin<span class=\\"token punctuation\\">(</span>train_df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span> <span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> <span class=\\"token string\\">\\"train\\"</span>\\n        df_with_splitmark <span class=\\"token operator\\">=</span> df_with_index<span class=\\"token punctuation\\">.</span>set_index<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">)</span>\\n\\n        tokenized_datadict <span class=\\"token operator\\">=</span> datadict<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">map</span><span class=\\"token punctuation\\">(</span><span class=\\"token keyword\\">lambda</span> x<span class=\\"token punctuation\\">:</span>self<span class=\\"token punctuation\\">.</span>tokenizer<span class=\\"token punctuation\\">(</span>x<span class=\\"token punctuation\\">[</span>text_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> padding<span class=\\"token operator\\">=</span><span class=\\"token string\\">'max_length'</span><span class=\\"token punctuation\\">,</span> truncation<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span> max_length<span class=\\"token operator\\">=</span><span class=\\"token number\\">512</span><span class=\\"token punctuation\\">,</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span>\\n                                        batched<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span>remove_columns <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"index\\"</span><span class=\\"token punctuation\\">,</span>text_column_name<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\n        tokenized_datadict<span class=\\"token punctuation\\">.</span>set_format<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"torch\\"</span><span class=\\"token punctuation\\">)</span>\\n        train_dataloader <span class=\\"token operator\\">=</span> DataLoader<span class=\\"token punctuation\\">(</span>tokenized_datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"train\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> shuffle<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span> batch_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">4</span><span class=\\"token punctuation\\">)</span>\\n        eval_dataloader <span class=\\"token operator\\">=</span> DataLoader<span class=\\"token punctuation\\">(</span>tokenized_datadict<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span> batch_size<span class=\\"token operator\\">=</span><span class=\\"token number\\">4</span><span class=\\"token punctuation\\">)</span>\\n\\n        <span class=\\"token keyword\\">return</span> train_dataloader<span class=\\"token punctuation\\">,</span>eval_dataloader<span class=\\"token punctuation\\">,</span>df_with_splitmark\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">train</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>train_dataloader<span class=\\"token punctuation\\">,</span>eval_dataloader<span class=\\"token punctuation\\">,</span>df_with_splitmark<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        train the model\\n        Args:\\n            train_dataloader:\\n            eval_dataloader:\\n\\n        Returns:\\n            train the self.model\\n\\n        \\"\\"\\"</span>\\n        optimizer <span class=\\"token operator\\">=</span> AdamW<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">.</span>parameters<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span> lr<span class=\\"token operator\\">=</span><span class=\\"token number\\">5e-5</span><span class=\\"token punctuation\\">)</span>\\n        num_training_steps <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>num_epochs <span class=\\"token operator\\">*</span> <span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>train_dataloader<span class=\\"token punctuation\\">)</span>\\n        lr_scheduler <span class=\\"token operator\\">=</span> get_scheduler<span class=\\"token punctuation\\">(</span>\\n            name<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"linear\\"</span><span class=\\"token punctuation\\">,</span> optimizer<span class=\\"token operator\\">=</span>optimizer<span class=\\"token punctuation\\">,</span> num_warmup_steps<span class=\\"token operator\\">=</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">,</span> num_training_steps<span class=\\"token operator\\">=</span>num_training_steps\\n        <span class=\\"token punctuation\\">)</span>\\n\\n        <span class=\\"token keyword\\">for</span> epoch <span class=\\"token keyword\\">in</span> <span class=\\"token builtin\\">range</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>num_epochs<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n            <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>epoch<span class=\\"token punctuation\\">)</span>\\n            self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">.</span>train<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">for</span> batch <span class=\\"token keyword\\">in</span> train_dataloader<span class=\\"token punctuation\\">:</span>\\n                batch <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">{</span>k<span class=\\"token punctuation\\">:</span> v<span class=\\"token punctuation\\">.</span>to<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>device<span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">for</span> k<span class=\\"token punctuation\\">,</span> v <span class=\\"token keyword\\">in</span> batch<span class=\\"token punctuation\\">.</span>items<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">}</span>\\n                outputs <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">(</span><span class=\\"token operator\\">**</span>batch<span class=\\"token punctuation\\">)</span>\\n                loss <span class=\\"token operator\\">=</span> outputs<span class=\\"token punctuation\\">.</span>loss\\n\\n                loss<span class=\\"token punctuation\\">.</span>backward<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n                optimizer<span class=\\"token punctuation\\">.</span>step<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n                lr_scheduler<span class=\\"token punctuation\\">.</span>step<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n                optimizer<span class=\\"token punctuation\\">.</span>zero_grad<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n\\n            self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">eval</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">for</span> batch <span class=\\"token keyword\\">in</span> eval_dataloader<span class=\\"token punctuation\\">:</span>\\n                batch <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">{</span>k<span class=\\"token punctuation\\">:</span> v<span class=\\"token punctuation\\">.</span>to<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>device<span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">for</span> k<span class=\\"token punctuation\\">,</span> v <span class=\\"token keyword\\">in</span> batch<span class=\\"token punctuation\\">.</span>items<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">}</span>\\n                <span class=\\"token keyword\\">with</span> torch<span class=\\"token punctuation\\">.</span>no_grad<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n                    outputs <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">(</span><span class=\\"token operator\\">**</span>batch<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># -1 would thus map to the last dimension, -2 to the preceding one, etc.</span>\\n                logits <span class=\\"token operator\\">=</span> outputs<span class=\\"token punctuation\\">.</span>logits\\n                predictions <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>argmax<span class=\\"token punctuation\\">(</span>logits<span class=\\"token punctuation\\">,</span> dim<span class=\\"token operator\\">=</span><span class=\\"token operator\\">-</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>epoch<span class=\\"token punctuation\\">,</span> loss<span class=\\"token punctuation\\">)</span>\\n\\n            eval_df <span class=\\"token operator\\">=</span> df_with_splitmark<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>df_with_splitmark<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"splitmark\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">==</span><span class=\\"token string\\">\\"test\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>copy<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n            <span class=\\"token keyword\\">for</span> i <span class=\\"token keyword\\">in</span> eval_df<span class=\\"token punctuation\\">.</span>index<span class=\\"token punctuation\\">:</span>\\n                result <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>predict<span class=\\"token punctuation\\">(</span>eval_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>i<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">[</span><span class=\\"token string\\">'text'</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\n                eval_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>i<span class=\\"token punctuation\\">,</span><span class=\\"token string\\">'predict_num'</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">=</span>result<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"num\\"</span><span class=\\"token punctuation\\">]</span>\\n                eval_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>i<span class=\\"token punctuation\\">,</span> <span class=\\"token string\\">'predict_labels'</span><span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> result<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span>\\n            confusion_df<span class=\\"token operator\\">=</span>self<span class=\\"token punctuation\\">.</span>evaluate<span class=\\"token punctuation\\">(</span>eval_df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">,</span>eval_df<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"predict_labels\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\n            self<span class=\\"token punctuation\\">.</span>save_all<span class=\\"token punctuation\\">(</span>df_with_splitmark<span class=\\"token punctuation\\">,</span>eval_df<span class=\\"token punctuation\\">,</span>confusion_df<span class=\\"token punctuation\\">,</span>epoch<span class=\\"token punctuation\\">)</span>\\n    \\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">one_stop_train</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>df<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        train, evaluate and save model&amp;tokenizer&amp;data\\n\\n        Args:\\n            df: original dataFrame\\n\\n        Returns:\\n            none\\n\\n        \\"\\"\\"</span>\\n        <span class=\\"token punctuation\\">[</span>train_dataloader<span class=\\"token punctuation\\">,</span>eval_dataloader<span class=\\"token punctuation\\">,</span>df_with_splitmark<span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>data_process<span class=\\"token punctuation\\">(</span>df<span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>train<span class=\\"token punctuation\\">(</span>train_dataloader<span class=\\"token punctuation\\">,</span>eval_dataloader<span class=\\"token punctuation\\">,</span>df_with_splitmark<span class=\\"token punctuation\\">)</span>\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">predict</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>text<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        predict using model\\n\\n        Args:\\n            text (str): one sentence\\n\\n        Returns (dict):\\n            \\"num\\": lable mapped to num\\n            \\"labels\\": num mapped to label\\n        \\"\\"\\"</span>\\n        self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">eval</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        data <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>tokenizer<span class=\\"token punctuation\\">(</span>text<span class=\\"token punctuation\\">,</span> truncation<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">,</span>max_length<span class=\\"token operator\\">=</span><span class=\\"token number\\">512</span><span class=\\"token punctuation\\">)</span>\\n        data <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">{</span>k<span class=\\"token punctuation\\">:</span> torch<span class=\\"token punctuation\\">.</span>tensor<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">[</span>v<span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>to<span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">.</span>device<span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">for</span> k<span class=\\"token punctuation\\">,</span> v <span class=\\"token keyword\\">in</span> data<span class=\\"token punctuation\\">.</span>items<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">}</span>\\n        <span class=\\"token keyword\\">with</span> torch<span class=\\"token punctuation\\">.</span>no_grad<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n            outputs <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">(</span><span class=\\"token operator\\">**</span>data<span class=\\"token punctuation\\">)</span>\\n        result <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>argmax<span class=\\"token punctuation\\">(</span>outputs<span class=\\"token punctuation\\">.</span>logits<span class=\\"token punctuation\\">,</span> dim<span class=\\"token operator\\">=</span><span class=\\"token operator\\">-</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>to<span class=\\"token punctuation\\">(</span><span class=\\"token string\\">\\"cpu\\"</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>numpy<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        result <span class=\\"token operator\\">=</span> <span class=\\"token builtin\\">int</span><span class=\\"token punctuation\\">(</span>result<span class=\\"token punctuation\\">[</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">return</span> <span class=\\"token punctuation\\">{</span><span class=\\"token string\\">\\"num\\"</span><span class=\\"token punctuation\\">:</span>result<span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"labels\\"</span><span class=\\"token punctuation\\">:</span>self<span class=\\"token punctuation\\">.</span>classLabel<span class=\\"token punctuation\\">.</span>int2str<span class=\\"token punctuation\\">(</span>result<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">}</span>\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">evaluate</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>labels_series<span class=\\"token punctuation\\">,</span>predictLabels_series<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token triple-quoted-string string\\">\\"\\"\\"\\n        create confusion matrix.\\n\\n        Args:\\n            labels_series (pandas.Series): marked labels\\n            predictLabel_series (pandas.Series): predict labels\\n\\n        Returns:\\n            confusion_df (pandas.DataFrame): confusion df that inclues all class recall and precision\\n\\n        \\"\\"\\"</span>\\n        confusion_df <span class=\\"token operator\\">=</span> pd<span class=\\"token punctuation\\">.</span>crosstab<span class=\\"token punctuation\\">(</span>labels_series<span class=\\"token punctuation\\">,</span>predictLabels_series<span class=\\"token punctuation\\">,</span> dropna<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">False</span><span class=\\"token punctuation\\">)</span>\\n        cha <span class=\\"token operator\\">=</span> <span class=\\"token builtin\\">set</span><span class=\\"token punctuation\\">(</span>confusion_df<span class=\\"token punctuation\\">.</span>columns<span class=\\"token punctuation\\">)</span> <span class=\\"token operator\\">^</span> <span class=\\"token builtin\\">set</span><span class=\\"token punctuation\\">(</span>confusion_df<span class=\\"token punctuation\\">.</span>index<span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">if</span> <span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>cha<span class=\\"token punctuation\\">)</span><span class=\\"token operator\\">&gt;</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">:</span>\\n            <span class=\\"token keyword\\">if</span><span class=\\"token punctuation\\">(</span><span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>confusion_df<span class=\\"token punctuation\\">.</span>index<span class=\\"token punctuation\\">)</span><span class=\\"token operator\\">&gt;</span><span class=\\"token builtin\\">len</span><span class=\\"token punctuation\\">(</span>confusion_df<span class=\\"token punctuation\\">.</span>columns<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n                confusion_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">,</span>cha<span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">=</span><span class=\\"token number\\">0</span>\\n            <span class=\\"token keyword\\">else</span><span class=\\"token punctuation\\">:</span>\\n                confusion_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span>cha<span class=\\"token punctuation\\">,</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">=</span><span class=\\"token number\\">0</span>\\n        confusion_df <span class=\\"token operator\\">=</span> confusion_df<span class=\\"token punctuation\\">.</span>reindex<span class=\\"token punctuation\\">(</span>index<span class=\\"token operator\\">=</span>self<span class=\\"token punctuation\\">.</span>classLabel<span class=\\"token punctuation\\">.</span>names<span class=\\"token punctuation\\">,</span>columns<span class=\\"token operator\\">=</span>self<span class=\\"token punctuation\\">.</span>classLabel<span class=\\"token punctuation\\">.</span>names<span class=\\"token punctuation\\">)</span>\\n        precision <span class=\\"token operator\\">=</span> confusion_df<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">apply</span><span class=\\"token punctuation\\">(</span><span class=\\"token keyword\\">lambda</span> x<span class=\\"token punctuation\\">:</span> x<span class=\\"token punctuation\\">[</span>x<span class=\\"token punctuation\\">.</span>name<span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">/</span> x<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">sum</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">if</span> x<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">sum</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token operator\\">!=</span><span class=\\"token number\\">0</span> <span class=\\"token keyword\\">else</span> <span class=\\"token number\\">0</span><span class=\\"token punctuation\\">)</span>\\n        recall <span class=\\"token operator\\">=</span> confusion_df<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">apply</span><span class=\\"token punctuation\\">(</span><span class=\\"token keyword\\">lambda</span> x<span class=\\"token punctuation\\">:</span> x<span class=\\"token punctuation\\">[</span>x<span class=\\"token punctuation\\">.</span>name<span class=\\"token punctuation\\">]</span> <span class=\\"token operator\\">/</span> x<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">sum</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span> <span class=\\"token keyword\\">if</span> x<span class=\\"token punctuation\\">.</span><span class=\\"token builtin\\">sum</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token operator\\">!=</span><span class=\\"token number\\">0</span> <span class=\\"token keyword\\">else</span> <span class=\\"token number\\">0</span><span class=\\"token punctuation\\">,</span>axis<span class=\\"token operator\\">=</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span>\\n        confusion_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span><span class=\\"token string\\">\\"precision\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">=</span> precision\\n        confusion_df<span class=\\"token punctuation\\">.</span>loc<span class=\\"token punctuation\\">[</span><span class=\\"token punctuation\\">:</span><span class=\\"token punctuation\\">,</span><span class=\\"token string\\">\\"recall\\"</span><span class=\\"token punctuation\\">]</span><span class=\\"token operator\\">=</span>recall\\n        <span class=\\"token keyword\\">return</span> confusion_df\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">save_all</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span>df_with_splitmark<span class=\\"token punctuation\\">,</span>eval_df<span class=\\"token punctuation\\">,</span>confusion_df<span class=\\"token punctuation\\">,</span>epoch<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        new_model_path_with_epoch <span class=\\"token operator\\">=</span> <span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>self<span class=\\"token punctuation\\">.</span>new_model_path<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">_</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>epoch<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">/\\"</span></span>\\n        path <span class=\\"token operator\\">=</span> Path<span class=\\"token punctuation\\">(</span><span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"./</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>new_model_path_with_epoch<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">\\"</span></span><span class=\\"token punctuation\\">)</span>\\n        path<span class=\\"token punctuation\\">.</span>mkdir<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>model<span class=\\"token punctuation\\">.</span>save_pretrained<span class=\\"token punctuation\\">(</span>new_model_path_with_epoch<span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>tokenizer<span class=\\"token punctuation\\">.</span>save_pretrained<span class=\\"token punctuation\\">(</span>new_model_path_with_epoch<span class=\\"token punctuation\\">)</span>\\n        df_with_splitmark<span class=\\"token punctuation\\">.</span>to_csv<span class=\\"token punctuation\\">(</span><span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>new_model_path_with_epoch<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">df_with_splitmark.csv\\"</span></span><span class=\\"token punctuation\\">)</span>\\n        eval_df<span class=\\"token punctuation\\">.</span>to_csv<span class=\\"token punctuation\\">(</span><span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>new_model_path_with_epoch<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">eval_df.csv\\"</span></span><span class=\\"token punctuation\\">)</span>\\n        confusion_df<span class=\\"token punctuation\\">.</span>to_csv<span class=\\"token punctuation\\">(</span><span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"</span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>new_model_path_with_epoch<span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">confusion_df.csv\\"</span></span><span class=\\"token punctuation\\">)</span>\\n\\n\\n\\n\\n\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}`);export{n as data};
