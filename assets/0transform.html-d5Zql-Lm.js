const e=JSON.parse('{"key":"v-5fe37942","path":"/4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2_3transform/0transform.html","title":"transform","lang":"zh-CN","frontmatter":{"description":"transform GPT GPT(Generative Pre-trained Transformer)全称是生成式预训练Transformer. 生成式(Generative): 就是像对话一样 预训练(Pre-trained): 预先进行训练好的模型, 使用时只需微调即可 transformer是关键, 包含4个部分: Embedding: 将词语用向量进行表示 Attention: 注意力机制 MLPs(Multi Layer Perceptron): Unembedding: 将向量映射到词语并给出预测值 alt text","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/4%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2_3transform/0transform.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"transform"}],["meta",{"property":"og:description","content":"transform GPT GPT(Generative Pre-trained Transformer)全称是生成式预训练Transformer. 生成式(Generative): 就是像对话一样 预训练(Pre-trained): 预先进行训练好的模型, 使用时只需微调即可 transformer是关键, 包含4个部分: Embedding: 将词语用向量进行表示 Attention: 注意力机制 MLPs(Multi Layer Perceptron): Unembedding: 将向量映射到词语并给出预测值 alt text"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-21T12:59:55.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-21T12:59:55.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"transform\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-21T12:59:55.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"GPT","slug":"gpt","link":"#gpt","children":[]},{"level":2,"title":"权重(weight)和数据(data)","slug":"权重-weight-和数据-data","link":"#权重-weight-和数据-data","children":[]},{"level":2,"title":"直观理解","slug":"直观理解","link":"#直观理解","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1740142795000,"updatedTime":1740142795000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.16,"words":347},"filePathRelative":"4机器学习/2_3transform/0transform.md","localizedDate":"2025年2月21日","excerpt":"<h1> transform</h1>\\n<h2> GPT</h2>\\n<p>GPT(Generative Pre-trained Transformer)全称是生成式预训练Transformer.</p>\\n<ol>\\n<li>\\n<p>生成式(Generative): 就是像对话一样</p>\\n</li>\\n<li>\\n<p>预训练(Pre-trained): 预先进行训练好的模型, 使用时只需微调即可</p>\\n</li>\\n<li>\\n<p>transformer是关键, 包含4个部分:</p>\\n<ol>\\n<li>Embedding: 将词语用向量进行表示</li>\\n<li>Attention: 注意力机制</li>\\n<li>MLPs(Multi Layer Perceptron):</li>\\n<li>Unembedding: 将向量映射到词语并给出预测值</li>\\n</ol>\\n<figure><figcaption>alt text</figcaption></figure>\\n</li>\\n</ol>","copyright":{"author":"王新宇"},"autoDesc":true}');export{e as data};
