const n=JSON.parse('{"key":"v-6ba7e5c4","path":"/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/5pytorch/%E5%BC%A0%E9%87%8F/zerograd.html","title":"zerograd","lang":"zh-CN","frontmatter":{"description":"zerograd import torch import torch.nn as nn class Simple(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3,1,bias=False) def forward(self, x): x = self.linear(x) return x model = Simple() for m in model.parameters(): print(m) # tensor([[-0.1925, -0.1296, 0.1730]], requires_grad=True) m.detach().fill_(0.1) # tensor赋值时必须先使用detach()方法脱离图(但是共享空间), 然后再赋值 print(m) # tensor([[0.1000, 0.1000, 0.1000]], requires_grad=True) criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=1.0) model.train() images = torch.ones(8, 3) targets = torch.ones(8, 1) output = model(images) print(output.shape) # torch.Size([8, 1]) loss = criterion(output, targets) print(model.linear.weight.grad) # None, 此时还未进行反向传播, 所以为None loss.backward() print(model.linear.weight.grad) # tensor([[-1.4000, -1.4000, -1.4000]]), 通过一次反向传播，计算出网络参数的导数， optimizer.step() print(model.linear.weight) #tensor([[1.5000, 1.5000, 1.5000]], requires_grad=True), 原始权重减去梯度, 对应上0.1 - (-1.4) = 1.5 optimizer.zero_grad() # 等价于调用`model.zero_grad()`, 但`optimizer.zero_grad()`更符合逻辑 print(model.linear.weight.grad) # tensor([[0., 0., 0.]]), 将权重的梯度重置为0, 不然会累加","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/5pytorch/%E5%BC%A0%E9%87%8F/zerograd.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"zerograd"}],["meta",{"property":"og:description","content":"zerograd import torch import torch.nn as nn class Simple(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3,1,bias=False) def forward(self, x): x = self.linear(x) return x model = Simple() for m in model.parameters(): print(m) # tensor([[-0.1925, -0.1296, 0.1730]], requires_grad=True) m.detach().fill_(0.1) # tensor赋值时必须先使用detach()方法脱离图(但是共享空间), 然后再赋值 print(m) # tensor([[0.1000, 0.1000, 0.1000]], requires_grad=True) criterion = nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=1.0) model.train() images = torch.ones(8, 3) targets = torch.ones(8, 1) output = model(images) print(output.shape) # torch.Size([8, 1]) loss = criterion(output, targets) print(model.linear.weight.grad) # None, 此时还未进行反向传播, 所以为None loss.backward() print(model.linear.weight.grad) # tensor([[-1.4000, -1.4000, -1.4000]]), 通过一次反向传播，计算出网络参数的导数， optimizer.step() print(model.linear.weight) #tensor([[1.5000, 1.5000, 1.5000]], requires_grad=True), 原始权重减去梯度, 对应上0.1 - (-1.4) = 1.5 optimizer.zero_grad() # 等价于调用`model.zero_grad()`, 但`optimizer.zero_grad()`更符合逻辑 print(model.linear.weight.grad) # tensor([[0., 0., 0.]]), 将权重的梯度重置为0, 不然会累加"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-25T10:13:52.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-25T10:13:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"zerograd\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-25T10:13:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[],"git":{"createdTime":1740478432000,"updatedTime":1740478432000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1,"words":299},"filePathRelative":"1机器学习/3分析工具/5pytorch/张量/zerograd.md","localizedDate":"2025年2月25日","excerpt":"<h1> zerograd</h1>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> torch\\n<span class=\\"token keyword\\">import</span> torch<span class=\\"token punctuation\\">.</span>nn <span class=\\"token keyword\\">as</span> nn\\n\\n<span class=\\"token keyword\\">class</span> <span class=\\"token class-name\\">Simple</span><span class=\\"token punctuation\\">(</span>nn<span class=\\"token punctuation\\">.</span>Module<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">__init__</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        <span class=\\"token builtin\\">super</span><span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>__init__<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n        self<span class=\\"token punctuation\\">.</span>linear <span class=\\"token operator\\">=</span> nn<span class=\\"token punctuation\\">.</span>Linear<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">3</span><span class=\\"token punctuation\\">,</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">,</span>bias<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">False</span><span class=\\"token punctuation\\">)</span>\\n\\n    <span class=\\"token keyword\\">def</span> <span class=\\"token function\\">forward</span><span class=\\"token punctuation\\">(</span>self<span class=\\"token punctuation\\">,</span> x<span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n        x <span class=\\"token operator\\">=</span> self<span class=\\"token punctuation\\">.</span>linear<span class=\\"token punctuation\\">(</span>x<span class=\\"token punctuation\\">)</span>\\n        <span class=\\"token keyword\\">return</span> x\\n\\n\\nmodel <span class=\\"token operator\\">=</span> Simple<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n\\n\\n<span class=\\"token keyword\\">for</span> m <span class=\\"token keyword\\">in</span> model<span class=\\"token punctuation\\">.</span>parameters<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">:</span>\\n    <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>m<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># tensor([[-0.1925, -0.1296,  0.1730]], requires_grad=True)</span>\\n    m<span class=\\"token punctuation\\">.</span>detach<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">.</span>fill_<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">0.1</span><span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># tensor赋值时必须先使用detach()方法脱离图(但是共享空间), 然后再赋值</span>\\n    <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>m<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># tensor([[0.1000, 0.1000, 0.1000]], requires_grad=True)</span>\\n\\ncriterion <span class=\\"token operator\\">=</span> nn<span class=\\"token punctuation\\">.</span>MSELoss<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\noptimizer <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>optim<span class=\\"token punctuation\\">.</span>SGD<span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>parameters<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span><span class=\\"token punctuation\\">,</span> lr<span class=\\"token operator\\">=</span><span class=\\"token number\\">1.0</span><span class=\\"token punctuation\\">)</span>\\n\\nmodel<span class=\\"token punctuation\\">.</span>train<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n\\nimages <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>ones<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">8</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">3</span><span class=\\"token punctuation\\">)</span>\\ntargets <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>ones<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">8</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">1</span><span class=\\"token punctuation\\">)</span>\\n\\noutput <span class=\\"token operator\\">=</span> model<span class=\\"token punctuation\\">(</span>images<span class=\\"token punctuation\\">)</span>\\n<span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>output<span class=\\"token punctuation\\">.</span>shape<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># torch.Size([8, 1])</span>\\n\\nloss <span class=\\"token operator\\">=</span> criterion<span class=\\"token punctuation\\">(</span>output<span class=\\"token punctuation\\">,</span> targets<span class=\\"token punctuation\\">)</span>\\n<span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>linear<span class=\\"token punctuation\\">.</span>weight<span class=\\"token punctuation\\">.</span>grad<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># None, 此时还未进行反向传播, 所以为None</span>\\nloss<span class=\\"token punctuation\\">.</span>backward<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n<span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>linear<span class=\\"token punctuation\\">.</span>weight<span class=\\"token punctuation\\">.</span>grad<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># tensor([[-1.4000, -1.4000, -1.4000]]), 通过一次反向传播，计算出网络参数的导数，</span>\\noptimizer<span class=\\"token punctuation\\">.</span>step<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>\\n<span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>linear<span class=\\"token punctuation\\">.</span>weight<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\">#tensor([[1.5000, 1.5000, 1.5000]], requires_grad=True), 原始权重减去梯度, 对应上0.1 - (-1.4) = 1.5</span>\\noptimizer<span class=\\"token punctuation\\">.</span>zero_grad<span class=\\"token punctuation\\">(</span><span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># 等价于调用`model.zero_grad()`, 但`optimizer.zero_grad()`更符合逻辑</span>\\n<span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span>model<span class=\\"token punctuation\\">.</span>linear<span class=\\"token punctuation\\">.</span>weight<span class=\\"token punctuation\\">.</span>grad<span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># tensor([[0., 0., 0.]]), 将权重的梯度重置为0, 不然会累加</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}');export{n as data};
