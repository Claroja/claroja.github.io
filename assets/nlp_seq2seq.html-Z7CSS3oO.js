import{_ as t}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as p,c as o,a as n,b as s,e as a}from"./app-XqA98pG8.js";const e="/assets/1--lyb4l0g.png",c="/assets/2-jaxppBYR.png",l="/assets/3-JyH7d5Ln.png",u="/assets/4-UgWeX8HA.png",i="/assets/5-emLktH4b.png",r="/assets/6-AGYix5L2.png",k="/assets/7-Ph2XK2CV.png",d="/assets/8-SRInzmPV.png",m="/assets/9-WaEml7za.png",h="/assets/10-8kiuHCtG.png",v="/assets/11-ydkj9aU2.png",b="/assets/12-X2vxZ-4U.png",f="/assets/13-SmBZgMlS.png",_={},g=n("h1",{id:"seq2seq",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#seq2seq","aria-hidden":"true"},"#"),s(" seq2seq")],-1),w=n("p",null,"seq2seq模型也称为Encoder-Decoder模型, 既该模型包含了Encoder(编码器)和Decoder(解码器).",-1),x=n("p",null,[s("考虑将日语翻译成英语, 如下图: "),n("img",{src:e,alt:"",loading:"lazy"}),s(' 首先对"吾輩は猫である"这句话进行编码, 然后将编码好的信息传递给解码器, 由解码器生成目标文本. '),n("img",{src:c,alt:"",loading:"lazy"}),s(" 编码器利用LSTM将时序数据转换为隐状态"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s(".隐状态"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("是LSTM层的最后一个隐状态, 其中编码了翻译输入文本所需的信息.隐状态"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("是一个固定长度的向量. "),n("img",{src:l,alt:"",loading:"lazy"}),s(" 解码器的结构和前面的神经网络相同, 不过也有稍许的差异, 就是LSTM会接收向量"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s(". 在前面的神经网络中, LSTM层不接收任何信息(接收初始化为0的向量).")],-1),q=a('<p><code>&lt;eos&gt;</code>在编码器中是生成文本的信号, 在解码器中是结束的信号. 在其他文献中也有使用<code>&lt;go&gt;</code>,<code>&lt;start&gt;</code>或者<code>_</code>作为分隔符的例子. <img src="'+u+'" alt="" loading="lazy"> 上图是seq2seq完整的结构, 由两个LSTM层构成. LSTM层的隐状态是编码器和解码器的&quot;桥梁&quot;.</p><h2 id="应用" tabindex="-1"><a class="header-anchor" href="#应用" aria-hidden="true">#</a> 应用</h2><p>将&quot;加法&quot;视为一个时序转换的问题, 具体来说, 在seq2seq学习后, 在输入&quot;57+5&quot;字符串后, 能正确回答&quot;62&quot;.</p><p>之前的word2vec中, 我们把文本以单词为单位进行分割. 而本节, 我们将不以单词为单位, 而是以字符为单位进行分割, 例如&quot;57+5&quot;会被处理为<code>[&#39;5&#39;,&#39;7&#39;,&#39;+&#39;,&#39;5&#39;]</code>.</p><p>注意不同的加法问题(&quot;57+5&quot;或&quot;628+521&quot;)及其回答(&quot;62&quot;或者&quot;1149&quot;)的字符数是不相同的. 在使用批数据进行学习时, 需要保证一个批次内各个样本的数形状是一致的. 在处理可变长数据时, 最简单的方法是使用padding. <img src="'+i+'" alt="" loading="lazy"> 本次将处理的问题限制在0~999的两个数的加法. 因此, 包括&quot;+&quot;在内, 输入的最大字符数是7. 另外, 加法的结果最大的是4个字符(最大为&quot;999+999=1998&quot;).</p><p>在输出的开始处加上了分隔符&quot;_&quot;, 使得输出数据的字符统一为5. 这个分隔符作为通知解码器开始生成文本的信号使用. 对于解码器的输出, 可在监督标签中插入表示字符输出结束的分隔符(&quot;<em>63</em>&quot;).</p><p>通过填充对齐数据的大小,可以处理可变长度的时序数据. 但是, 因为使用了填充, seq2seq需要处理原本不存在的填充用字符, 所以如果追求严谨, 使用填充时需要向seq2seq添加一些填充专用的处理, 比如: 在解码器中输入填充时, 不应计算其损失(向Softmax with loss添加mask功能来解决). 再比如: 在编码器中输入填充时, LSTM层应按原样输出上一时刻的输入.</p><h2 id="seq2seq的实现" tabindex="-1"><a class="header-anchor" href="#seq2seq的实现" aria-hidden="true">#</a> seq2seq的实现</h2><h3 id="encoder类" tabindex="-1"><a class="header-anchor" href="#encoder类" aria-hidden="true">#</a> Encoder类</h3>',9),y=n("p",null,[s("Encoder类接收字符串, 将其转换为向量"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s(".")],-1),M=n("figure",null,[n("img",{src:r,alt:"",tabindex:"0",loading:"lazy"}),n("figcaption")],-1),T=n("p",null,"Encoder类由Embedding层和LSTM层组成. Embedding层将字符ID转化为字符向量, 然后将字符向量输入LSTM层.",-1),z=n("p",null,[s("LSTM层向右输出隐藏状态和记忆单元, 向上输出隐状态. 这里因为上方不存在层, 所以丢弃LSTM层向上的输出. 在编码器处理完最后一个字符后, 输出LSTM层的隐状态"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s(". 然后, 这个隐藏状态"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("被传递给解码器.")],-1),L=a(`<p>编码器只将LSTM的隐状态传递给解码器. 尽管也可以把LSTM的记忆单元传递给解码器, 但我们通常不太会把LSTM的记忆单元传递给其他层. 这是因为, LSTM的记忆单元被设计为只给自身使用.</p><p>python实现:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        rn <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn
        embed_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wx <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>D<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wh <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> TimeEmbedding<span class="token punctuation">(</span>embed_W<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> TimeLSTM<span class="token punctuation">(</span>lstm_Wx<span class="token punctuation">,</span> lstm_Wh<span class="token punctuation">,</span> lstm_b<span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>params <span class="token operator">+</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>params
        self<span class="token punctuation">.</span>grads <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>grads <span class="token operator">+</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>grads
        self<span class="token punctuation">.</span>hs <span class="token operator">=</span> <span class="token boolean">None</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>初始化方法接收vocab_size, wordvec_size, hidden_size这3个参数.vocab_size是词汇量, 字符的种类. 这里总共有13种字符(数字0~9,&quot;+&quot;,&quot; &quot;,&quot;_&quot;). wordvec_size对应字符向量的维数, hidden_size对应于LSTM层的隐状态的维数.</p><p>这个初始化方法进行权重参数的初始化和层的生成. 最后, 将权重参数和梯度分别归纳在成员变量params和grads的列表中. 因为这次并不保持Time LSTM层的状态, 所以设定stateful=False.</p><p>在之前的语言模型处理的是只有一个长时序数据的问题. 那时我们设定Time LSTM层的参数stateful=True, 以在保持隐状态的同时, 处理长时序数据. 而这次是由多个短时序数据的问题. 因此, 针对每个问题重设LSTM的隐状态(0向量).</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    xs <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
    hs <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>hs <span class="token operator">=</span> hs
    <span class="token keyword">return</span> hs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
<span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dh<span class="token punctuation">)</span><span class="token punctuation">:</span>
    dhs <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hs<span class="token punctuation">)</span>
    dhs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> dh
    dout <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dhs<span class="token punctuation">)</span>
    dout <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
    <span class="token keyword">return</span> dout
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>编码器的正向传播调佣Time Embedding层和Time LSTM层的<code>forward()</code>方法, 然后取出Time LSTM层的最后一个时刻的隐状态, 将它作为编码器的<code>forward()</code>方法的输出.</p><p>在编码器的反向传播中, LSTM层的最后一个隐状态的梯度是<code>dh</code>, 这个<code>dh</code>是从解码器传来的梯度. 在反向传播的实现中, 先生成元素为0的张量<code>dhs</code>, 再将<code>dh</code>存放放到这个<code>dhs</code>中的对应位置. 剩下的就是调用Time Embedding层和TIme LSTM层的<code>backward()</code>方法.</p><h2 id="decoder类" tabindex="-1"><a class="header-anchor" href="#decoder类" aria-hidden="true">#</a> Decoder类</h2>`,10),S=n("p",null,[s("Decoder类接收Encoder类输出的"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s(", 输出目标字符串. "),n("img",{src:k,alt:"",loading:"lazy"}),s(' 这里使用了监督数据"_62"进行学习, 输入的数据是'),n("code",null,"['_','6','2',' ']"),s(", 对应的输出是"),n("code",null,"['6','2',' ',' ']"),s(".")],-1),H=a('<p>在使用RNN进行文本生成时, 学习时和生成时的数据输入方法不同.</p><ul><li>在学习时, 因为已经知道正确解, 所以可以整体地输入时序方向上的数据.</li><li>相对地, 在推理时(生成新字符串时), 则只能输入第1个通知开始的分隔符(本次为&quot;_&quot;).然后, 输出1个字符, 并将这个字符作为下一个输入, 如此重复该过程.</li></ul><p>另外, 因为和生成文本不同, 我们需要确定性的文字, 所以使用argmax, 另外没有使用Softmax层, 而是从Affine层中选择最大的作为最终结果, 如下图: <img src="'+d+'" alt="" loading="lazy"></p><p>在解码器中, 学习和生成时处理Softmax层的方式是不一样的. 因此, Softmax with loss层交给伺候实现的seq2seq类处理, Decoder类仅承担Time Softmax with loss层之前的部分. <img src="'+m+`" alt="" loading="lazy"> python实现:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        rn <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn
        embed_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wx <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>D<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wh <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        affine_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        affine_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> TimeEmbedding<span class="token punctuation">(</span>embed_W<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> TimeLSTM<span class="token punctuation">(</span>lstm_Wx<span class="token punctuation">,</span> lstm_Wh<span class="token punctuation">,</span> lstm_b<span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>affine <span class="token operator">=</span> TimeAffine<span class="token punctuation">(</span>affine_W<span class="token punctuation">,</span> affine_b<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>params <span class="token operator">+=</span> layer<span class="token punctuation">.</span>params
            self<span class="token punctuation">.</span>grads <span class="token operator">+=</span> layer<span class="token punctuation">.</span>grads
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>set_state<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        <span class="token keyword">return</span> score
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dscore<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dscore<span class="token punctuation">)</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        dh <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>dh
        <span class="token keyword">return</span> dh
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Decoder类在学习时和在生成文本时的行为不同. 上面的<code>forward()</code>方法是假定在学习时使用的.将生成文本的方法实现为<code>generate()</code></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> h<span class="token punctuation">,</span> start_id<span class="token punctuation">,</span> sample_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sampled <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    sample_id <span class="token operator">=</span> start_id
    self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>set_state<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>sample_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>sample_id<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        sample_id <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>score<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        sampled<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>sample_id<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> sampled
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><code>generate()</code>方法有三个参数:</p><ul><li>隐状态h</li><li>最开始输入的字符串ID start_id</li><li>生成的字符数量sample_size 这里重复如下操作: 输入一个字符, 选择Affine层输出的得分中最大值的字符ID.</li></ul>`,9),D=n("p",null,[s("这次问题中, 需要将编码器的输出"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("设定给解码器的Time LSTM.此时, 通过设置TIme LSTM层为stateful, 可以不重设隐藏状态, 在保持编码器的"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("同时, 进行正向传播.")],-1),W=a(`<p>python实现完整的seq2seq:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Seq2seq</span><span class="token punctuation">(</span>BaseModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> TimeSoftmaxWithLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>params <span class="token operator">+</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>params
        self<span class="token punctuation">.</span>grads <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>grads <span class="token operator">+</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>grads
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> ts<span class="token punctuation">)</span><span class="token punctuation">:</span>
        decoder_xs<span class="token punctuation">,</span> decoder_ts <span class="token operator">=</span> ts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> ts<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>decoder_xs<span class="token punctuation">,</span> h<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>score<span class="token punctuation">,</span> decoder_ts<span class="token punctuation">)</span>
        <span class="token keyword">return</span> loss
    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        dh <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dout<span class="token punctuation">)</span>
        dout <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>dh<span class="token punctuation">)</span>
        <span class="token keyword">return</span> dout
    <span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> start_id<span class="token punctuation">,</span> sample_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        sampled <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_id<span class="token punctuation">,</span> sample_size<span class="token punctuation">)</span>
        <span class="token keyword">return</span> sampled
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="反转输入数据-reverse" tabindex="-1"><a class="header-anchor" href="#反转输入数据-reverse" aria-hidden="true">#</a> 反转输入数据(Reverse)</h2><p><img src="`+h+'" alt="" loading="lazy"> 注意这里是直接反转, 而不是将之前的数据和反转的数据同时作为输入.</p><p>为什么反转数据后, 学习进展快, 精度提高了呢?直观上的解释是:</p><p>吾輩 は 猫 で ある 到 I am a cat 这个问题中:</p><p>&quot;吾輩&quot;和&quot;单词I&quot;之间有转换关系, 此时从&quot;吾輩&quot;到&quot;I&quot;的路程必须经过&quot;は&quot;&quot;猫&quot;&quot;で&quot;&quot;ある&quot;这4个单词的LSTM层. 因此在反向传播时, 梯度从&quot;I&quot;抵达&quot;吾輩&quot;, 也要受到这个距离的影响.</p><p>反转输入语句, 也就是变成&quot;ある で 猫 は 吾輩&quot;, 此时&quot;吾輩&quot;和&quot;单词I&quot;彼此相邻, 梯度可以直接传递, 传播变得更容易. 不过, 单词之间的&quot;平均&quot;距离并不会发生改变.</p><h2 id="偷窥-peeky" tabindex="-1"><a class="header-anchor" href="#偷窥-peeky" aria-hidden="true">#</a> 偷窥(Peeky)</h2>',9),V=n("p",null,[s("当前的seq2seq只有最开始时刻的LSTM层利用了"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])])],-1),E=n("figure",null,[n("img",{src:v,alt:"",tabindex:"0",loading:"lazy"}),n("figcaption")],-1),I=n("p",null,[s("通过偷窥(Peeky)改进后, 将"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s("分配给解码器的其它层.")],-1),N=n("figure",null,[n("img",{src:b,alt:"",tabindex:"0",loading:"lazy"}),n("figcaption")],-1),A=n("p",null,[s("如上图, 编码器的"),n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("semantics",null,[n("mrow",null,[n("mi",null,"h")]),n("annotation",{encoding:"application/x-tex"},"h")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6944em"}}),n("span",{class:"mord mathnormal"},"h")])])]),s('分配给所有时刻的Affine层和LSTM层. 即是其他层也能"偷窥"到编码信息, 这个改进的解码器称为Peeky Decoder. 称使用了Peeky Decoder的seq2seq为 Peeky seq2seq。')],-1),P=a('<p>有两个向量同时被输入到了LSTM层和Affine层, 这实际上表示两个向量的拼贴(concatenate), 如下图:</p><figure><img src="'+f+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>python实现如下:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PeekyDecoder</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        V<span class="token punctuation">,</span> D<span class="token punctuation">,</span> H <span class="token operator">=</span> vocab_size<span class="token punctuation">,</span> wordvec_size<span class="token punctuation">,</span> hidden_size
        rn <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn
        embed_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>V<span class="token punctuation">,</span> D<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wx <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span> H <span class="token operator">+</span> D <span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H <span class="token operator">+</span> D<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_Wh <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        lstm_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        affine_W <span class="token operator">=</span> <span class="token punctuation">(</span>rn<span class="token punctuation">(</span> H <span class="token operator">+</span> H <span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>H <span class="token operator">+</span> H<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        affine_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>V<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token string">&#39;f&#39;</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> TimeEmbedding<span class="token punctuation">(</span>embed_W<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> TimeLSTM<span class="token punctuation">(</span>lstm_Wx<span class="token punctuation">,</span> lstm_Wh<span class="token punctuation">,</span> lstm_b<span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>affine <span class="token operator">=</span> TimeAffine<span class="token punctuation">(</span>affine_W<span class="token punctuation">,</span> affine_b<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>params<span class="token punctuation">,</span> self<span class="token punctuation">.</span>grads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>embed<span class="token punctuation">,</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">,</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>params <span class="token operator">+=</span> layer<span class="token punctuation">.</span>params
            self<span class="token punctuation">.</span>grads <span class="token operator">+=</span> layer<span class="token punctuation">.</span>grads
        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> <span class="token boolean">None</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">,</span> h<span class="token punctuation">)</span><span class="token punctuation">:</span>
        N<span class="token punctuation">,</span> T <span class="token operator">=</span> xs<span class="token punctuation">.</span>shape
        N<span class="token punctuation">,</span> H <span class="token operator">=</span> h<span class="token punctuation">.</span>shape
        self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>set_state<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
        hs <span class="token operator">=</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>h<span class="token punctuation">,</span> T<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>N<span class="token punctuation">,</span> T<span class="token punctuation">,</span> H<span class="token punctuation">)</span>
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>hs<span class="token punctuation">,</span> out<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        out <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>hs<span class="token punctuation">,</span> out<span class="token punctuation">)</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
        score <span class="token operator">=</span> self<span class="token punctuation">.</span>affine<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>out<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cache <span class="token operator">=</span> H
        <span class="token keyword">return</span> score
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="seq2seq的应用" tabindex="-1"><a class="header-anchor" href="#seq2seq的应用" aria-hidden="true">#</a> seq2seq的应用</h2><ul><li>机器翻译：将“一种语言的文本”转换为“另一种语言的文本”</li><li>自动摘要：将“一个长文本”转换为“短摘要”</li><li>问答系统：将“问题”转换为“答案”</li><li>邮件自动回复：将“接收到的邮件文本”转换为“回复文本”</li></ul>`,6),B=[g,w,x,q,y,M,T,z,L,S,H,D,W,V,E,I,N,A,P];function R(U,X){return p(),o("div",null,B)}const F=t(_,[["render",R],["__file","nlp_seq2seq.html.vue"]]);export{F as default};
