import{_ as a}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as n,c as s,d as t}from"./app-j3zK2x6_.js";const o={},e=t('<h1 id="autogradqs-tutorial" tabindex="-1"><a class="header-anchor" href="#autogradqs-tutorial" aria-hidden="true">#</a> autogradqs_tutorial</h1><p>在训练网络上时, 我们使用<code>back propagation</code>的算法, parameters(模型的weights), 通过<code>gradient</code> of the <code>loss function</code>来调整. PyTorch通过内置的<code>torch.autograd</code>来计算<code>gradients</code> 考虑一层的神经网络, 输入是$$x$$, parameters是$$w$$和$$b$$:</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch\n\nx <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># input tensor</span>\ny <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># expected output</span>\nw <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\nb <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>\nz <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token operator">+</span>b\nloss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>z<span class="token punctuation">,</span> y<span class="token punctuation">)</span>\n</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>上面的代码创建的网络结构如下: <img src="https://pytorch.org/tutorials/_images/comp-graph.png" alt="" loading="lazy"></p><p class="katex-block"><span class="katex-error" title="ParseError: KaTeX parse error: Can&#39;t use function &#39;$&#39; in math mode at position 3: \nw$̲$和$$b$$是paramet…" style="color:#cc0000;"> w$$和$$b$$是parameters, 我们需要optimize.通过loss function来计算这些variables的gradients, 所以要`requires_grad`为`true` 每个tensor都有一个`grad_fn`来计算`forward direction`和`backward propagation` ```python print(&#39;Gradient function for z =&#39;, z.grad_fn) print(&#39;Gradient function for loss =&#39;, loss.grad_fn) ``` ## Computing Gradients 我们需要计算$$x$$和$$y$$对应的梯度$$\\frac{\\partial loss}{\\partial w}$$和$$\\frac{\\partial loss}{\\partial b}$$.为了计算这些derivatives, 我们可以使用`loss.backward()`,然后通过`w.grad`和`b.grad`来获取具体的值. ```python loss.backward() print(w.grad) print(b.grad) ``` 注意, 1. 只有parameters设置了`requires_grad=true` 2. 只能在graph中使用一次`backward()`来进行gradient calculations.如果需要多次计算, 则需要`backward(retain_graph=True)` ## Disabling Gradient Tracking 默认所有`requires_grad=True`的`tensor`都会保留计算的历史和支持gradient计算.但是有些情况不需要这些属性, 比如计算`forward`可以提高计算速度, 或`finetuning a pretrained network`需要frozen parameters的时候. ```python z = torch.matmul(x, w)+b print(z.requires_grad) with torch.no_grad(): z = torch.matmul(x, w)+b print(z.requires_grad) ``` 另一种方式是: ```python z = torch.matmul(x, w)+b z_det = z.detach() print(z_det.requires_grad) ``` ## Optional Reading: Tensor Gradients and Jacobian Products 参考: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#automatic-differentiation-with-torch-autograd</span></p>',5),r=[e];function p(c,i){return n(),s("div",null,r)}const d=a(o,[["render",p],["__file","basics_autogradqs_tutorial.html.vue"]]);export{d as default};
