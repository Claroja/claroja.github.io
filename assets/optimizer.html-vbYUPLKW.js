const a=JSON.parse('{"key":"v-3ec9cf5a","path":"/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3optimize/optimizer.html","title":"optimizer","lang":"zh-CN","frontmatter":{"description":"optimizer 参数更新 神经网络的学习的目的是找到使损失函数的值尽可能小的参数, 这个过程称为最优化(optimization). SGD 使用参数的梯度, 沿着梯度的方向更新参数, 并且重复这个步骤多次, 从而逐渐靠近最优参数, 这个过程称为随机梯度下降法(stochastic gradient descent, SGD). 公式可以写成 W←W−η∂L∂W W \\\\leftarrow W - \\\\eta\\\\frac{\\\\partial L}{\\\\partial W} W←W−η∂W∂L​","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3optimize/optimizer.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"optimizer"}],["meta",{"property":"og:description","content":"optimizer 参数更新 神经网络的学习的目的是找到使损失函数的值尽可能小的参数, 这个过程称为最优化(optimization). SGD 使用参数的梯度, 沿着梯度的方向更新参数, 并且重复这个步骤多次, 从而逐渐靠近最优参数, 这个过程称为随机梯度下降法(stochastic gradient descent, SGD). 公式可以写成 W←W−η∂L∂W W \\\\leftarrow W - \\\\eta\\\\frac{\\\\partial L}{\\\\partial W} W←W−η∂W∂L​"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://claroja.github.io/"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-24T12:46:58.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"optimizer"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-24T12:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"optimizer\\",\\"image\\":[\\"https://claroja.github.io/\\"],\\"dateModified\\":\\"2025-02-24T12:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"参数更新","slug":"参数更新","link":"#参数更新","children":[]},{"level":2,"title":"SGD","slug":"sgd","link":"#sgd","children":[]},{"level":2,"title":"SGD的缺点","slug":"sgd的缺点","link":"#sgd的缺点","children":[]},{"level":2,"title":"Momentum","slug":"momentum","link":"#momentum","children":[]},{"level":2,"title":"AdaGrad","slug":"adagrad","link":"#adagrad","children":[]},{"level":2,"title":"adam","slug":"adam","link":"#adam","children":[]},{"level":2,"title":"RMSProp","slug":"rmsprop","link":"#rmsprop","children":[]}],"git":{"createdTime":1740401218000,"updatedTime":1740401218000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":5.46,"words":1639},"filePathRelative":"2机器学习/1算法原理/4深度学习/3optimize/optimizer.md","localizedDate":"2025年2月24日","excerpt":"<h1> optimizer</h1>\\n<h2> 参数更新</h2>\\n<p>神经网络的学习的目的是找到使损失函数的值尽可能小的参数, 这个过程称为最优化(optimization).</p>\\n<h2> SGD</h2>\\n<p>使用参数的梯度, 沿着梯度的方向更新参数, 并且重复这个步骤多次, 从而逐渐靠近最优参数, 这个过程称为随机梯度下降法(stochastic gradient descent, SGD).\\n公式可以写成</p>\\n<p class=\\"katex-block\\"><span class=\\"katex-display\\"><span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\" display=\\"block\\"><semantics><mrow><mi>W</mi><mo>←</mo><mi>W</mi><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant=\\"normal\\">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant=\\"normal\\">∂</mi><mi>W</mi></mrow></mfrac></mrow><annotation encoding=\\"application/x-tex\\">\\nW \\\\leftarrow W - \\\\eta\\\\frac{\\\\partial L}{\\\\partial W} \\n</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6833em;\\"></span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.13889em;\\">W</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span><span class=\\"mrel\\">←</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.7667em;vertical-align:-0.0833em;\\"></span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.13889em;\\">W</span><span class=\\"mspace\\" style=\\"margin-right:0.2222em;\\"></span><span class=\\"mbin\\">−</span><span class=\\"mspace\\" style=\\"margin-right:0.2222em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:2.0574em;vertical-align:-0.686em;\\"></span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.03588em;\\">η</span><span class=\\"mord\\"><span class=\\"mopen nulldelimiter\\"></span><span class=\\"mfrac\\"><span class=\\"vlist-t vlist-t2\\"><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:1.3714em;\\"><span style=\\"top:-2.314em;\\"><span class=\\"pstrut\\" style=\\"height:3em;\\"></span><span class=\\"mord\\"><span class=\\"mord\\" style=\\"margin-right:0.05556em;\\">∂</span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.13889em;\\">W</span></span></span><span style=\\"top:-3.23em;\\"><span class=\\"pstrut\\" style=\\"height:3em;\\"></span><span class=\\"frac-line\\" style=\\"border-bottom-width:0.04em;\\"></span></span><span style=\\"top:-3.677em;\\"><span class=\\"pstrut\\" style=\\"height:3em;\\"></span><span class=\\"mord\\"><span class=\\"mord\\" style=\\"margin-right:0.05556em;\\">∂</span><span class=\\"mord mathnormal\\">L</span></span></span></span><span class=\\"vlist-s\\">​</span></span><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.686em;\\"><span></span></span></span></span></span><span class=\\"mclose nulldelimiter\\"></span></span></span></span></span></span></p>","copyright":{"author":"王新宇"},"autoDesc":true}');export{a as data};
