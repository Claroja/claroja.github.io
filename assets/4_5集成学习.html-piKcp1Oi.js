import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as t,c as e,e as a}from"./app-A10JnHE0.js";const n="/assets/variance-bias-8AmW6DgC.png",l="/assets/袋装法-6nrqt1V7.png",o="/assets/袋装法2-FybhyKZ5.png",s="/assets/提升法原理-Tjrp7qiN.png",r="/assets/提升法原理2-JRKNc6Cg.png",g="/assets/提升法原理3-eOCNM8oS.png",d="/assets/提升法原理4-5LRfNSNg.png",h="/assets/提升法原理5-NOuwiQck.png",c={},p=a('<p>集成学习概述 抽样技术：</p><ul><li>训练数据上的抽样方法</li><li>输入字段上的抽样方法 袋装法：</li><li>通用袋装法、随机森林 提升法：</li><li>Adaboost、GBDT、xgboost、LightGBM</li></ul><h2 id="集成学习概述" tabindex="-1"><a class="header-anchor" href="#集成学习概述" aria-hidden="true">#</a> 集成学习概述</h2><p>如果要对其进行分类，可以大致划分为模型融合、机器学习元算法。</p><ol><li>模型融合是一个再学习的过程，将训练出的强学习器组合起来进一步提高性能。不同算法, 同一个数据.</li><li>机器学习元算法则是弱学习器的组合。同一个算法, 不同的数据.</li></ol><p>模型融合： 多数法（Max Voting） 平均法（Averaging） 加权平均法（Weighted Averaging） 堆叠法（Stacking） 混合法（Blending）</p><p>机器学习元算法：</p><ol><li>袋装法（Bagging）。 <ol><li>袋装通用法（Bagging Meta-Estimator）</li><li>随机森林（Random Forest）。</li></ol></li><li>提升法（Boosting）。 <ol><li>Adaboost 算法</li><li>XGBoost 算法</li><li>Gradient Boost 算法。</li></ol></li></ol><h2 id="模型融合-多数法-max-voting-。" tabindex="-1"><a class="header-anchor" href="#模型融合-多数法-max-voting-。" aria-hidden="true">#</a> 模型融合-多数法（Max Voting）。</h2><p>多数法通常用于分类问题。 在该技术中，使用多个模型来对每个数据进行预测。 每个模型的预测被视为“投票”。 我们从大多数模型得到的预测被用作最终预测。 例如，当你让 5 位同事评价你的电影时（5 分）；我们假设其中三位将它评为 4，而其中两位将它评为 5。 由于大多数评分为 4，最终评级的结果就为 4（取众数 Mode）。</p><h2 id="模型融合-平均法-averaging-。" tabindex="-1"><a class="header-anchor" href="#模型融合-平均法-averaging-。" aria-hidden="true">#</a> 模型融合-平均法（Averaging）。</h2><p>与多数法类似，但采用所有模型的平均预测值来进行最终预测。 平均法可用于回归问题或分类问题中的概率预测。 -(5+4+5+4+4)/5=4.4。</p><h2 id="模型融合-加权平均法-weighted-averaging-。" tabindex="-1"><a class="header-anchor" href="#模型融合-加权平均法-weighted-averaging-。" aria-hidden="true">#</a> 模型融合-加权平均法（Weighted Averaging）。</h2><p>这是平均法的扩展。根据模型的重要性分配不同的权重。 例如，如果你的两个同事都是评论者，而另一些同事没有这方面的经验，那么这两个朋友的答案相较于其他人更为重要。 -(5<em>0.23)+(4</em>0.23)+(5<em>0.18)+(4</em>0.18)+(4<em>0.18)+(4</em>0.18)=4.41。</p><h2 id="模型融合-堆叠法-stacking" tabindex="-1"><a class="header-anchor" href="#模型融合-堆叠法-stacking" aria-hidden="true">#</a> 模型融合-堆叠法（Stacking）</h2><p>堆叠法使用来自多个模型（例如决策树、KNN 或 SVM）的预测来构建新模型。</p><h2 id="模型融合-混合法-blending" tabindex="-1"><a class="header-anchor" href="#模型融合-混合法-blending" aria-hidden="true">#</a> 模型融合-混合法(Blending)</h2><p>混合法与堆叠法类似，但混合法仅使用验证集来重新建模并进行预测</p><h2 id="机器学习元算法" tabindex="-1"><a class="header-anchor" href="#机器学习元算法" aria-hidden="true">#</a> 机器学习元算法</h2><p>机器学习元算法是把许多“弱”学习器通过算法集成起来变为“强”分类器。 根据个体学习器的生成方式，可分为 Bagging 和 Boosting 两大类。 Bagging 的个体学习器不存在依赖关系，可同时对样本随机采样并行化生成个体学习器。 Boosting 的个体学习器之间存在强的依赖关系：在基于前面模型的训练结果（误差）生成新的模型，所以必须串行化生成。 Bagging 的方法对样本随机取样产生多个独立模型，然后平均所有模型的预测值，旨在减小方差（Variance）——要稳。 Boosting 的方法拟合前一个模型的误差，旨在减小偏差（Bias）——要准。</p><figure><img src="'+n+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="抽样技术" tabindex="-1"><a class="header-anchor" href="#抽样技术" aria-hidden="true">#</a> 抽样技术</h2><p>SamplingTechniques</p><ul><li><p>By manipulating the training set（通过操纵训练集）。</p><ul><li>In this approach, the sampling distribution determines how likely it is that an example will be selected for training（在这种方法中，抽样分布决定了一个样本被选入训练的可能性）。</li><li>A classifier is then built from each training set using a particular learning algorithm（然后使用特定的学习算法从每个训练集中构建一个分类器）。 Bagging and Boosting are two examples（袋装法和提升法是两个例子）。</li></ul></li><li><p>By manipulating the input features（通过操纵输入特征）。</p><ul><li>In this approach, a subset of input features is chosen to form each training set（在这种方法中，选择输入特征的一个子集来形成每个训练集）。</li><li>This approach works very well with datasets that contain highly redundant features（这种方法在包含高度冗余特征的数据集上效果很好）。</li><li>Random forest is an example of this type and use decision trees as its base classifier（随机森林是这种类型的一个例子，它使用决策树作为其基本分类器）。</li></ul></li></ul><p>袋装法Bagging</p><p>Consider the following dataset (y is the target)</p><p>表格内容为：</p><table><thead><tr><th>x</th><th>0.1</th><th>0.2</th><th>0.3</th><th>0.4</th><th>0.5</th><th>0.6</th><th>0.7</th><th>0.8</th><th>0.9</th><th>1</th></tr></thead><tbody><tr><td>y</td><td>1</td><td>1</td><td>1</td><td>-1</td><td>-1</td><td>-1</td><td>-1</td><td>1</td><td>1</td><td>（此处 y 值缺失）</td></tr></tbody></table><ul><li>Suppose we apply a classifier that induces only one-level binary tree, the best split is either χ ⩽ 0.35 or χ ⩽ 0.75（假设我们应用一个仅能生成一层二叉树的分类器，最佳分割点要么是 χ ⩽ 0.35，要么是 χ ⩽ 0.75）。</li><li>Either way, the accuracy is at most 70%（无论哪种情况，准确率最多为 70%）。</li></ul><figure><img src="'+l+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+o+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><ul><li>From preceding example, even though each base classifier is just a only one-level binary tree, combining the classifiers can lead to a decision tree of depth 2（从前一个例子来看，即使每个基本分类器只是一个一层的二叉树，组合这些分类器可以得到一个深度为 2 的决策树）。</li><li>Since every example has an equal probability of being selected, bagging does not focus on any particular instance of the training data（由于每个样本被选中的概率相等，袋装法不会专注于训练数据中的任何特定实例）。</li><li>It is therefore less susceptible to model overfitting when applied to noise data（因此，在应用于有噪声的数据时，它不太容易出现模型过拟合）。</li></ul><h2 id="随机森林" tabindex="-1"><a class="header-anchor" href="#随机森林" aria-hidden="true">#</a> 随机森林</h2><ul><li>It is specifically designed for decision tree. Randomize the algorithm, not the training data。</li><li>Attribute Selection for Decision Tree: Don&#39;t pick the best, pick randomly from the k best attributes。</li></ul><h2 id="提升法boosting" tabindex="-1"><a class="header-anchor" href="#提升法boosting" aria-hidden="true">#</a> 提升法Boosting</h2><ul><li><p>Boosting 源起于 1990 年代，由美国计算机科学家沙皮尔（Robert Schapire）所提出。</p></li><li><p>1995 年，亚夫·佛洛依（Yoav Freund）和沙皮尔再次改进了 Boosting 算法的一些缺点，提出了 AdaBoost 算法。 在当时，Adaboost、随机森林与支持向量机，三者并驾齐驱，算是最好的分类器。</p></li><li><p>2001 年，美国史丹佛大学的统计学教授，杰罗姆傅利曼（Jerome H. Friedman）提出了 GBDT（梯度提升决策树，Gradient Boosting Decision Tree）的概念，主要是集成许多弱（决策树）分类器，以发展出最优模型的方法。</p></li><li><p>由于 Boosting 是利用迭代来找到优化过后的模型，一旦碰到数据集非常复杂时，背后需要上千上万次的迭代运算，造成计算瓶颈，容易拖慢计算机的计算效率。 面对这项问题，2014 年：华盛顿大学博士生陈天奇开发出 XGBoost。</p></li><li><p>XGBoost 的全名为 eXtreme Gradient Boosting（极限梯度提升），大幅提升模型的训练速度与预测的精准度。</p></li><li><p>2016 年底，微软的 DMTK 团队开发出训练效率更高、准确率更高，而且计算机内存使用更低的 LightGBM（Light Gradient Boosting Machine，LGBM）高效算法。</p></li><li><p>Boosting is an iterative procedure used to adaptively change the distribution of training example, so that the base classifiers will focus on examples that hard to classify。</p></li><li><p>Unlike bagging, boosting assigns a weight to each training example and may adaptively change the weight at the end of each boosting round。</p></li><li><p>Initially, the examples are assigned equal weight, 1/N, so that they are equally likely to be chosen for training.</p></li><li><p>A sample is drawn according to the sample distribution to obtain a new training set.</p></li><li><p>Next, a classifier is induced from the new training set and used to classify all the examples in the original data.</p></li></ul><figure><img src="'+s+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+r+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+g+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+d+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><ul><li>With AdaBoost, it perfectly classifies all the examples in the training data.</li><li>Because of its tendency to focus on training examples that are wrongly classified, the boosting technique can be quite susceptible to overfitting. New Boosting Model: XGBoost. XGBoost 全称为 Extreme Gradient Boosting，</li></ul><figure><img src="'+h+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><ul><li>GBDT（Gradient Boosting Decision Tree）的原理是：</li><li>首先使用训练集和样本真值（即标准答案）训练一棵树，使用这棵树预测训练集，得到每个样本的预测值，由于预测值与真值存在偏差，所以二者相减可以得到残差。</li><li>接下来训练第二棵树，此时使用残差代替真值作为标准答案，两棵树训练完成后，可以再次得到每个样本的残差。</li><li>然后进一步训练第三棵树，以此类推，树的总棵数可以人为指定，也可以监控某些指标如验证集上的误差来停止训练。</li></ul>',43),u=[p];function f(m,b){return t(),e("div",null,u)}const B=i(c,[["render",f],["__file","4_5集成学习.html.vue"]]);export{B as default};
