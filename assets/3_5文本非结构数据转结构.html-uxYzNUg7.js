import{_ as t}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as e,c as i,a}from"./app-7knaTE3M.js";const o="/assets/词袋模型1-xwzprUod.png",s="/assets/词袋模型2-ZZ6HM5yG.png",n="/assets/词袋模型3-oM09ef-h.png",r="/assets/PCA-izVcuPa2.png",l="/assets/PCA2-TVgAOO5w.png",g="/assets/PCA3-3WO3W6cl.png",p="/assets/PCA4-J-E5NVEe.png",c="/assets/PCA5-iodoQtEV.png",d="/assets/PCA6-AsK6sGJV.png",f="/assets/PCA7-G96PDy8j.png",m="/assets/MF1-n9WSCEAU.png",u="/assets/SVD1-ajke7aK3.png",h="/assets/SVD2-ma3uqn6M.png",x="/assets/SVD3-ZjnearR_.png",_="/assets/SVD4-e4-cmPRg.png",y="/assets/SVD5-qiIfc_Vp.png",w="/assets/SVD6-0bS6MTwG.png",v="/assets/SVD7-QjFsWCWi.png",b="/assets/glove1-ngrMF4xK.png",z="/assets/glove2-plGt7-Q6.png",V="/assets/glove3-6IhKD6Jf.png",W="/assets/glove4-s1C1BdCw.png",C="/assets/glove5-iswyO5yx.png",D="/assets/glove6--kymTCNf.png",k="/assets/NN1-fllJDUSq.png",P="/assets/NN2-EVRX7Nqw.png",A="/assets/NN3-nyaDLP-s.png",S="/assets/NN4-UFT3y0iy.png",G="/assets/NN5-Z44Y-GGy.png",E="/assets/NN6-gwp8mrqB.png",M="/assets/w2v1-kLY3uSkM.png",N="/assets/w2v2-iJe-SGr3.png",q="/assets/softmax-xECCgqOY.png",T="/assets/w2v3-XCqkTIxx.png",B="/assets/w2v4-EMRMRUUi.png",O="/assets/w2v5-uoVvqYa7.png",I="/assets/w2v6-7BVqV9_g.png",K="/assets/w2v7-_KPvQv6k.png",R="/assets/w2v8-z1CGEJ7X.png",F="/assets/w2v9-bUoCzq8E.png",U="/assets/w2v10-YY02XYMq.png",L="/assets/w2v11-WdH5rLye.png",Y={},J=a('<p>词袋模型(Bag ofWords,BoW)</p><p>无监督的词嵌入模型-Glove</p><ol><li>PCA及矩阵分解</li><li>词嵌入模型Glove</li></ol><p>有监督的词嵌入模型-Word2Vec</p><ol><li>神经网络</li><li>词嵌入模型skip-Gram</li><li>词嵌入模型skip-Gram</li></ol><h2 id="词的表示法" tabindex="-1"><a class="header-anchor" href="#词的表示法" aria-hidden="true">#</a> 词的表示法</h2><p>screte Representation：一个维度只表达一个Word的信息，一个Word的信息也只由一个维度表达. bag of Words (One-hot Encoding)</p><p>Distributed Representation :描述语言的向量是定长的(Fix-Length），一个语义信息可能由多个维度共同决定，，一个维度也可能决定着多方面的语义信息. Glove &amp; Word2Vec</p><p>·D1:只要在日本有看过可口可乐广告的观众朋友一定对他留下了深刻的印象。 ·KW:日本、可口可乐、广告</p><p>·D2:伊拉克已连续第四天将 原油倾入波斯湾。距科威特海岸 五 公里处的海岛 石油转运站 的原油也倾入 波斯湾 及沙特阿拉伯海岸 ·D2:伊拉克已连续第四天将 原油倾入波斯湾。距科威特海岸 五 公里处的海岛 石油转运站 的原油也倾入 波斯湾 及沙特阿拉伯海岸</p><p>·D3:波斯湾沙特阿拉伯海岸，廿六日平静的海面，闪着发亮的层层油污。原油已流往科威特南方一百廿公里 附近的卡夫吉 ·D3:波斯湾沙特阿拉伯海岸，廿六日平静的海面，闪着发亮的层层油污。原油已流往科威特南方一百廿公里 附近的卡夫吉</p><p>·D4:新世航一号渔船蝶血案上的我国籍船员... ·KW:新世航一号、渔船、蝶血案</p><p>·D5:侦办新世航渔船碟血案的项目人员指出， ·KW:新世航、渔船、蝶血案</p><p>·AIlKW:日本 可口可乐 广告 伊拉克 原油 波斯湾 科威特 沙特阿拉伯 新世航一号 渔船 蝶血案 新世航</p><p><img src="'+o+'" alt="词袋模型1" loading="lazy"><img src="'+s+'" alt="词袋模型2" loading="lazy"><img src="'+n+'" alt="词袋模型3" loading="lazy"></p><p>·One-hot Encoding的缺点是假设所有的Word互相独立，导致向量维度太大 ·Glove &amp;Word2Vec的Word Vector就比较稠密(Dense)，大大缩短向量长度，不再像One-hot Encoding一样稀疏 ·DistributedRepresentation的wordVectors的方法也叫做WordEmbedding方法(词嵌入）：将Word的语义信息嵌入Fix-Length的向量中</p><h2 id="无监督的词嵌入模型-glove" tabindex="-1"><a class="header-anchor" href="#无监督的词嵌入模型-glove" aria-hidden="true">#</a> 无监督的词嵌入模型-Glove</h2><p>PCA</p><p>MatrixFactorization</p><ol><li>SVD</li><li>Glove</li></ol><figure><img src="'+r+'" alt="PCA" tabindex="0" loading="lazy"><figcaption>PCA</figcaption></figure><figure><img src="'+l+'" alt="PCA2" tabindex="0" loading="lazy"><figcaption>PCA2</figcaption></figure><figure><img src="'+g+'" alt="PCA3" tabindex="0" loading="lazy"><figcaption>PCA3</figcaption></figure><figure><img src="'+p+'" alt="PCA4" tabindex="0" loading="lazy"><figcaption>PCA4</figcaption></figure><figure><img src="'+c+'" alt="PCA5" tabindex="0" loading="lazy"><figcaption>PCA5</figcaption></figure><p>对词进行压缩 <img src="'+d+'" alt="alt text" loading="lazy"></p><figure><img src="'+f+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="matrix-factorization" tabindex="-1"><a class="header-anchor" href="#matrix-factorization" aria-hidden="true">#</a> Matrix Factorization</h2><figure><img src="'+m+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>Eigenvalues及Eigen Vectors的求解仅适用于方正矩阵，User-ItemRatingMatrix通常不是方正矩阵</p><p>Eigenvalues及EigenVectors常常会求出复数解，在运用上有困难</p><p>奇异值分解(Singular Value Decomposition,SVD)可解决这两个问题</p><p>·SVD是一个常见的矩阵分解算法。奇异值类似主成分，我们往往取TopK个奇异值就能够表示绝大部分信息量，因此SVD经常拿来做损失较小的有损压缩 ·SVD是一个常见的矩阵分解算法。奇异值类似主成分，我们往往取TopK个奇异值就能够表示绝大部分信息量，因此SVD经常拿来做损失较小的有损压缩 <img src="'+u+'" alt="alt text" loading="lazy"></p><figure><img src="'+h+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+x+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+_+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>SVD的缺点, 不能有空值. Conventional SVDisundefinedwhenknowledge aboutthe matrixisincomplete This often raises difficulties due to the high portion of missing values caused bysparseness in the user-item rating matrix <img src="'+y+'" alt="alt text" loading="lazy"></p><p>PU分解可以解决这个问题</p><figure><img src="'+w+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>还原回原矩阵.</p><figure><img src="'+v+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h2 id="the-glove-model" tabindex="-1"><a class="header-anchor" href="#the-glove-model" aria-hidden="true">#</a> The GloVe Model</h2><ol><li>GloVe(GlobalVectors forWordRepresentation)是史丹福大学发表的一种WordEmbedding方法，它看起来很New但其实是l旧瓶换新酒</li><li>GloVe发表于Word2Vec之后，它借鉴Word2Vec的Pair-Wise的方法以及其他一些Trick来进行传统矩阵分解运算进而得到WordVectors</li></ol><p>词贡献矩阵(Word-Context Co-occurrencee Matrix) <img src="'+b+'" alt="alt text" loading="lazy"></p><figure><img src="'+z+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+V+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+W+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+C+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+D+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>文章的词嵌入表示就是将词向量直接相加然后除以词数</p><h2 id="word2vec" tabindex="-1"><a class="header-anchor" href="#word2vec" aria-hidden="true">#</a> Word2Vec</h2><p>BP Neural1 Network</p><p><img src="'+k+'" alt="text" loading="lazy"><img src="'+P+'" alt="text" loading="lazy"></p><p>线性不可分:An Example:XOR</p><p><img src="'+A+'" alt="text" loading="lazy"><img src="'+S+'" alt="text" loading="lazy"><img src="'+G+'" alt="text" loading="lazy"></p><p>由InputLayer到HiddenLayer的过程，目的在于坐标转换及降低分析维度 <img src="'+E+'" alt="text" loading="lazy"></p><p>Word2vec Models</p><p>There are two types of Word2Vec models</p><ol><li>The Skip-Gram Model</li><li>The CBOW Model</li></ol><h3 id="the-skip-gram-model" tabindex="-1"><a class="header-anchor" href="#the-skip-gram-model" aria-hidden="true">#</a> The Skip-Gram Model</h3><figure><img src="'+M+'" alt="text" tabindex="0" loading="lazy"><figcaption>text</figcaption></figure><p>只用了组合函数, 没用激活函数</p><figure><img src="'+N+'" alt="text" tabindex="0" loading="lazy"><figcaption>text</figcaption></figure><p>There is no activation function on the hidden layer neurons, but the outputneurons use softmax</p><p>When training this network on word pairs, the input is a one-hot vectorrepresenting the input word and the training output is also a one-hot vectorrepresenting the output word</p><p>But when you evaluate the trained network on an input word, the outputvector will actually be a probability distribution.</p><p>For our example, we&#39;re learning word vectors with 300 features. So thehidden layer is going to be represented by a weight matrix with 10,000rows (one for every word in our vocabulary) and 300 columns (one forevery hidden neuron)</p><figure><img src="'+q+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>在隐藏层取出word emmbedding</p><figure><img src="'+T+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>在输出层预测 <img src="'+B+'" alt="alt text" loading="lazy"></p><p>· If two different words have very similar “contexts&quot; (that is, what words. arelikely to appear around them), then our model needs to output very similarresuIts for these two words. · If two different words have very similar “contexts&quot; (that is, what words. arelikely to appear around them), then our model needs to output very similarresuIts for these two words. ·And what does it mean for two words to have similar contexts? I think youcould expect that synonyms like “intelligent” and “smart” would have verysimilar contexts. Or that words that are related, like “engine” and&quot;transmission&quot;, would probably have similar contexts as well ·And what does it mean for two words to have similar contexts? I think youcould expect that synonyms like “intelligent” and “smart” would have verysimilar contexts. Or that words that are related, like “engine” and&quot;transmission&quot;, would probably have similar contexts as well</p><figure><img src="'+O+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+I+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+K+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><h3 id="the-cbow-model" tabindex="-1"><a class="header-anchor" href="#the-cbow-model" aria-hidden="true">#</a> The CBOW Model</h3><figure><img src="'+R+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><p>Because there are multiple contextual words, we average their corresponding word vectors, constructed by the multiplication of the input vector and the matrix W. Because the averaging stage smoothes over a lot of the distributional information, some people believe the CBOW model is better for small dataset.</p><figure><img src="'+F+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+U+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure><figure><img src="'+L+'" alt="alt text" tabindex="0" loading="lazy"><figcaption>alt text</figcaption></figure>',81),X=[J];function j(Q,Z){return e(),i("div",null,X)}const tt=t(Y,[["render",j],["__file","3_5文本非结构数据转结构.html.vue"]]);export{tt as default};
