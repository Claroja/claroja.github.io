const a=JSON.parse('{"key":"v-0c2b7427","path":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/%E5%AE%98%E7%BD%91%E7%BF%BB%E8%AF%91/basics_autogradqs_tutorial.html","title":"autogradqs_tutorial","lang":"zh-CN","frontmatter":{"description":"autogradqs_tutorial 在训练网络上时, 我们使用back propagation的算法, parameters(模型的weights), 通过gradient of the loss function来调整. PyTorch通过内置的torch.autograd来计算gradients 考虑一层的神经网络, 输入是$$x$$, parameters是$$w$$和$$b$$: import torch x = torch.ones(5) # input tensor y = torch.zeros(3) # expected output w = torch.randn(5, 3, requires_grad=True) b = torch.randn(3, requires_grad=True) z = torch.matmul(x, w)+b loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/pytorch/%E5%AE%98%E7%BD%91%E7%BF%BB%E8%AF%91/basics_autogradqs_tutorial.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"autogradqs_tutorial"}],["meta",{"property":"og:description","content":"autogradqs_tutorial 在训练网络上时, 我们使用back propagation的算法, parameters(模型的weights), 通过gradient of the loss function来调整. PyTorch通过内置的torch.autograd来计算gradients 考虑一层的神经网络, 输入是$$x$$, parameters是$$w$$和$$b$$: import torch x = torch.ones(5) # input tensor y = torch.zeros(3) # expected output w = torch.randn(5, 3, requires_grad=True) b = torch.randn(3, requires_grad=True) z = torch.matmul(x, w)+b loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-25T13:46:58.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2023-11-25T13:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"autogradqs_tutorial\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2023-11-25T13:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[],"git":{"createdTime":1700920018000,"updatedTime":1700920018000,"contributors":[{"name":"claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.29,"words":388},"filePathRelative":"机器学习/pytorch/官网翻译/basics_autogradqs_tutorial.md","localizedDate":"2023年11月25日","excerpt":"<h1> autogradqs_tutorial</h1>\\n<p>在训练网络上时, 我们使用<code>back propagation</code>的算法, parameters(模型的weights), 通过<code>gradient</code> of the <code>loss function</code>来调整.\\nPyTorch通过内置的<code>torch.autograd</code>来计算<code>gradients</code>\\n考虑一层的神经网络, 输入是$$x$$, parameters是$$w$$和$$b$$:</p>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token keyword\\">import</span> torch\\n\\nx <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>ones<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">5</span><span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># input tensor</span>\\ny <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>zeros<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">3</span><span class=\\"token punctuation\\">)</span>  <span class=\\"token comment\\"># expected output</span>\\nw <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>randn<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">5</span><span class=\\"token punctuation\\">,</span> <span class=\\"token number\\">3</span><span class=\\"token punctuation\\">,</span> requires_grad<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">)</span>\\nb <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>randn<span class=\\"token punctuation\\">(</span><span class=\\"token number\\">3</span><span class=\\"token punctuation\\">,</span> requires_grad<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">True</span><span class=\\"token punctuation\\">)</span>\\nz <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>matmul<span class=\\"token punctuation\\">(</span>x<span class=\\"token punctuation\\">,</span> w<span class=\\"token punctuation\\">)</span><span class=\\"token operator\\">+</span>b\\nloss <span class=\\"token operator\\">=</span> torch<span class=\\"token punctuation\\">.</span>nn<span class=\\"token punctuation\\">.</span>functional<span class=\\"token punctuation\\">.</span>binary_cross_entropy_with_logits<span class=\\"token punctuation\\">(</span>z<span class=\\"token punctuation\\">,</span> y<span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}');export{a as data};
