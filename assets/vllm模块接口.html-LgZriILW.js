import{_ as n}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as s,c as a,a as t}from"./app-nD1Z-e8V.js";const e={},p=t(`<div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># SPDX-License-Identifier: Apache-2.0</span>

<span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

<span class="token comment"># Sample prompts.</span>
prompts <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;Hello, my name is&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;The president of the United States is&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;The capital of France is&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;The future of AI is&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>
<span class="token comment"># Create a sampling params object.</span>
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.8</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>

<span class="token comment"># Create an LLM.</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;facebook/opt-125m&quot;</span><span class="token punctuation">)</span>
<span class="token comment"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
<span class="token comment"># that contain the prompt, generated text, and other information.</span>
outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> sampling_params<span class="token punctuation">)</span>
<span class="token comment"># Print the outputs.</span>
<span class="token keyword">for</span> output <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>
    prompt <span class="token operator">=</span> output<span class="token punctuation">.</span>prompt
    generated_text <span class="token operator">=</span> output<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Prompt: </span><span class="token interpolation"><span class="token punctuation">{</span>prompt<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">, Generated text: </span><span class="token interpolation"><span class="token punctuation">{</span>generated_text<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="api" tabindex="-1"><a class="header-anchor" href="#api" aria-hidden="true">#</a> API</h2><p><code>class vllm.SamplingParams()</code></p><ol><li>n: 一个提示返回几个序列</li><li>best_of: 生成多少个最优序列, best_of 必须大于或等于 n。</li><li>presence_penalty: token是否在之前序列中出现过惩罚, 值大于 0 鼓励模型使用新的标记，值小于 0 鼓励模型重复标记。</li><li>frequency_penalty：token的频率惩罚, 值大于 0 鼓励模型使用新的标记，值小于 0 鼓励模型重复标记。</li><li>repetition_penalty: token是否出现在提示和目前生成的文本中对其进行惩罚。值大于 1 鼓励模型使用新的标记，值小于 1 鼓励模型重复标记。</li><li>temperature: 值较低时使模型更具确定性，值较高时使模型更具随机性</li><li>top_p：根据累计概率设定token的考虑范围, 设置为 1 时考虑所有标记。</li><li>top_k: 根据数量设定token的考虑范围, 设置为 -1 时考虑所有标记。</li><li>min_p: token小于此概率将不出现</li><li>seed：随机种子。</li><li>stop：遇到该词停止</li><li>stop_token_ids：生成的停止词id列表</li><li>bad_words：不允许生成的单词列表。</li><li>include_stop_str_in_output：是否在输出文本中包含停止字符串。</li><li>ignore_eos：是否忽略 EOS 标记并在生成 EOS 标记后继续生成标记。</li><li>max_tokens：每个输出序列要生成的最大标记数量。</li><li>min_tokens：在可以生成 EOS 或 stop_token_ids 之前，每个输出序列要生成的最小标记数量。</li><li>logprobs：每个输出标记要返回的对数概率数量。设置为 None 时，不返回概率。</li></ol><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考" aria-hidden="true">#</a> 参考</h2><ol><li>https://github.com/datawhalechina/self-llm/blob/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md</li><li>https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/basic/basic.py</li></ol>`,6),o=[p];function i(l,c){return s(),a("div",null,o)}const m=n(e,[["render",i],["__file","vllm模块接口.html.vue"]]);export{m as default};
