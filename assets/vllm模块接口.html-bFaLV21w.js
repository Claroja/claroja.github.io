const n=JSON.parse('{"key":"v-61671363","path":"/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/5%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/vllm%E6%A8%A1%E5%9D%97%E6%8E%A5%E5%8F%A3.html","title":"","lang":"zh-CN","frontmatter":{"description":"# SPDX-License-Identifier: Apache-2.0 from vllm import LLM, SamplingParams # Sample prompts. prompts = [ \\"Hello, my name is\\", \\"The president of the United States is\\", \\"The capital of France is\\", \\"The future of AI is\\", ] # Create a sampling params object. sampling_params = SamplingParams(temperature=0.8, top_p=0.95) # Create an LLM. llm = LLM(model=\\"facebook/opt-125m\\") # Generate texts from the prompts. The output is a list of RequestOutput objects # that contain the prompt, generated text, and other information. outputs = llm.generate(prompts, sampling_params) # Print the outputs. for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\\"Prompt: {prompt!r}, Generated text: {generated_text!r}\\")","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/5%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/vllm%E6%A8%A1%E5%9D%97%E6%8E%A5%E5%8F%A3.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:description","content":"# SPDX-License-Identifier: Apache-2.0 from vllm import LLM, SamplingParams # Sample prompts. prompts = [ \\"Hello, my name is\\", \\"The president of the United States is\\", \\"The capital of France is\\", \\"The future of AI is\\", ] # Create a sampling params object. sampling_params = SamplingParams(temperature=0.8, top_p=0.95) # Create an LLM. llm = LLM(model=\\"facebook/opt-125m\\") # Generate texts from the prompts. The output is a list of RequestOutput objects # that contain the prompt, generated text, and other information. outputs = llm.generate(prompts, sampling_params) # Print the outputs. for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\\"Prompt: {prompt!r}, Generated text: {generated_text!r}\\")"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-25T10:13:52.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-25T10:13:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-25T10:13:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"API","slug":"api","link":"#api","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1740478432000,"updatedTime":1740478432000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.79,"words":537},"filePathRelative":"1机器学习/2应用场景/5大语言模型/vllm模块接口.md","localizedDate":"2025年2月25日","excerpt":"<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code><span class=\\"token comment\\"># SPDX-License-Identifier: Apache-2.0</span>\\n\\n<span class=\\"token keyword\\">from</span> vllm <span class=\\"token keyword\\">import</span> LLM<span class=\\"token punctuation\\">,</span> SamplingParams\\n\\n<span class=\\"token comment\\"># Sample prompts.</span>\\nprompts <span class=\\"token operator\\">=</span> <span class=\\"token punctuation\\">[</span>\\n    <span class=\\"token string\\">\\"Hello, my name is\\"</span><span class=\\"token punctuation\\">,</span>\\n    <span class=\\"token string\\">\\"The president of the United States is\\"</span><span class=\\"token punctuation\\">,</span>\\n    <span class=\\"token string\\">\\"The capital of France is\\"</span><span class=\\"token punctuation\\">,</span>\\n    <span class=\\"token string\\">\\"The future of AI is\\"</span><span class=\\"token punctuation\\">,</span>\\n<span class=\\"token punctuation\\">]</span>\\n<span class=\\"token comment\\"># Create a sampling params object.</span>\\nsampling_params <span class=\\"token operator\\">=</span> SamplingParams<span class=\\"token punctuation\\">(</span>temperature<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.8</span><span class=\\"token punctuation\\">,</span> top_p<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.95</span><span class=\\"token punctuation\\">)</span>\\n\\n<span class=\\"token comment\\"># Create an LLM.</span>\\nllm <span class=\\"token operator\\">=</span> LLM<span class=\\"token punctuation\\">(</span>model<span class=\\"token operator\\">=</span><span class=\\"token string\\">\\"facebook/opt-125m\\"</span><span class=\\"token punctuation\\">)</span>\\n<span class=\\"token comment\\"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>\\n<span class=\\"token comment\\"># that contain the prompt, generated text, and other information.</span>\\noutputs <span class=\\"token operator\\">=</span> llm<span class=\\"token punctuation\\">.</span>generate<span class=\\"token punctuation\\">(</span>prompts<span class=\\"token punctuation\\">,</span> sampling_params<span class=\\"token punctuation\\">)</span>\\n<span class=\\"token comment\\"># Print the outputs.</span>\\n<span class=\\"token keyword\\">for</span> output <span class=\\"token keyword\\">in</span> outputs<span class=\\"token punctuation\\">:</span>\\n    prompt <span class=\\"token operator\\">=</span> output<span class=\\"token punctuation\\">.</span>prompt\\n    generated_text <span class=\\"token operator\\">=</span> output<span class=\\"token punctuation\\">.</span>outputs<span class=\\"token punctuation\\">[</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">]</span><span class=\\"token punctuation\\">.</span>text\\n    <span class=\\"token keyword\\">print</span><span class=\\"token punctuation\\">(</span><span class=\\"token string-interpolation\\"><span class=\\"token string\\">f\\"Prompt: </span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>prompt<span class=\\"token conversion-option punctuation\\">!r</span><span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">, Generated text: </span><span class=\\"token interpolation\\"><span class=\\"token punctuation\\">{</span>generated_text<span class=\\"token conversion-option punctuation\\">!r</span><span class=\\"token punctuation\\">}</span></span><span class=\\"token string\\">\\"</span></span><span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div>","copyright":{"author":"王新宇"},"autoDesc":true}');export{n as data};
