const e=JSON.parse('{"key":"v-73cf87ff","path":"/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/5pytorch/%E5%BC%A0%E9%87%8F/requiresgrad.html","title":"requiresgrad","lang":"zh-CN","frontmatter":{"description":"requiresgrad tensor被创建的时候, 默认tensor.requires_grad为False, 既不需要求导. 在tensor的计算中, 如果输入中, 有一个输入需要求导tensor.requires_grad=True, 输出一定会需要求导. 只有当所有输入都不需要求导的时候, 输出才会不需要求导. 比如我们喂给模型的数据(DataLoader取出的数据), 这些数据是不需要求导的. 网络的输出也是不需要求导的. 但是, 损失值loss是自动求导的, 因为虽然输入的训练数据是默认不求导的, 而model中的所有参数是默认求导的, 正是, 只要输入一个需要求导, 那么输出的网络结果必定会需要求导.","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/5pytorch/%E5%BC%A0%E9%87%8F/requiresgrad.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"requiresgrad"}],["meta",{"property":"og:description","content":"requiresgrad tensor被创建的时候, 默认tensor.requires_grad为False, 既不需要求导. 在tensor的计算中, 如果输入中, 有一个输入需要求导tensor.requires_grad=True, 输出一定会需要求导. 只有当所有输入都不需要求导的时候, 输出才会不需要求导. 比如我们喂给模型的数据(DataLoader取出的数据), 这些数据是不需要求导的. 网络的输出也是不需要求导的. 但是, 损失值loss是自动求导的, 因为虽然输入的训练数据是默认不求导的, 而model中的所有参数是默认求导的, 正是, 只要输入一个需要求导, 那么输出的网络结果必定会需要求导."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-24T12:46:58.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-24T12:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"requiresgrad\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-24T12:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"Finetuning","slug":"finetuning","link":"#finetuning","children":[]},{"level":2,"title":"torch.no_grad","slug":"torch-no-grad","link":"#torch-no-grad","children":[]}],"git":{"createdTime":1740401218000,"updatedTime":1740401218000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.73,"words":518},"filePathRelative":"2机器学习/3分析工具/5pytorch/张量/requiresgrad.md","localizedDate":"2025年2月24日","excerpt":"<h1> requiresgrad</h1>\\n<p>tensor被创建的时候, 默认<code>tensor.requires_grad</code>为<code>False</code>, 既不需要求导.</p>\\n<p>在tensor的计算中, 如果输入中, 有一个输入需要求导<code>tensor.requires_grad=True</code>, 输出一定会需要求导. 只有当所有输入都不需要求导的时候, 输出才会不需要求导.</p>\\n<p>比如我们喂给模型的数据(<code>DataLoader</code>取出的数据), 这些数据是不需要求导的. 网络的输出也是不需要求导的. 但是, 损失值loss是自动求导的, 因为虽然输入的训练数据是默认不求导的, 而model中的所有参数是默认求导的, 正是, 只要输入一个需要求导, 那么输出的网络结果必定会需要求导.</p>","copyright":{"author":"王新宇"},"autoDesc":true}');export{e as data};
