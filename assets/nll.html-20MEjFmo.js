import{_ as s}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as t,a as n,b as e,e as o}from"./app-SnI5rGHA.js";const p="/assets/1-sT8ZxmHA.png",l={},c=n("h1",{id:"nll",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#nll","aria-hidden":"true"},"#"),e(" nll")],-1),i=n("p",null,"Negative log-likelihood is a loss function used in multi-class classification. In practice, the softmax function is used in tandem with the negative log-likelihood (NLL).",-1),r=n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("mi",null,"L"),n("mo",{stretchy:"false"},"("),n("mi",null,"y"),n("mo",{stretchy:"false"},")"),n("mo",null,"="),n("mo",null,"−"),n("mi",null,"l"),n("mi",null,"o"),n("mi",null,"g"),n("mo",{stretchy:"false"},"("),n("mi",null,"y"),n("mo",{stretchy:"false"},")")]),n("annotation",{encoding:"application/x-tex"}," L(y) = -log(y) ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord mathnormal"},"L"),n("span",{class:"mopen"},"("),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),n("span",{class:"mclose"},")"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),n("span",{class:"mord"},"−"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),n("span",{class:"mord mathnormal"},"o"),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"g"),n("span",{class:"mopen"},"("),n("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"y"),n("span",{class:"mclose"},")")])])])])],-1),u=o('<p>where y is a prediction corresponding to the true label, after the Softmax Activation Function was applied. The loss for a mini-batch is computed by taking the mean or sum of all items in the batch.</p><figure><img src="'+p+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
torch<span class="token punctuation">.</span>set_printoptions<span class="token punctuation">(</span>sci_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
logits <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.45</span><span class="token punctuation">,</span> <span class="token number">0.23</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
softmax_probs <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
nll <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>softmax_probs<span class="token punctuation">[</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># range(len(labels))是样本的id</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([0.0382])
&#39;&#39;&#39;</span>
nll <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">&#39;none&#39;</span><span class="token punctuation">)</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>softmax_probs<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token triple-quoted-string string">&#39;&#39;&#39;
tensor([0.0382])
&#39;&#39;&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>refs: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/ https://notesbylex.com/negative-log-likelihood.html</p>`,4),m=[c,i,r,u];function d(k,h){return a(),t("div",null,m)}const v=s(l,[["render",d],["__file","nll.html.vue"]]);export{v as default};
