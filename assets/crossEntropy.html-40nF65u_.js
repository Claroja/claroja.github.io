const t=JSON.parse(`{"key":"v-5b3fb956","path":"/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2lossfunction/crossEntropy.html","title":"crossEntropy","lang":"zh-CN","frontmatter":{"description":"crossEntropy Categorical Cross-Entropy Loss Function applies the Softmax Activation Function to a model's output (logits) before applying the Negative Log-Likelihood function. P=softmax(O) P = softmax(O) P=softmax(O)","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/4%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/2lossfunction/crossEntropy.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"crossEntropy"}],["meta",{"property":"og:description","content":"crossEntropy Categorical Cross-Entropy Loss Function applies the Softmax Activation Function to a model's output (logits) before applying the Negative Log-Likelihood function. P=softmax(O) P = softmax(O) P=softmax(O)"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://claroja.github.io/"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-25T10:13:52.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"crossEntropy"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-25T10:13:52.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"crossEntropy\\",\\"image\\":[\\"https://claroja.github.io/\\"],\\"dateModified\\":\\"2025-02-25T10:13:52.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"softmax loss vs cross entropy loss","slug":"softmax-loss-vs-cross-entropy-loss","link":"#softmax-loss-vs-cross-entropy-loss","children":[]}],"git":{"createdTime":1740478432000,"updatedTime":1740478432000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":1.03,"words":309},"filePathRelative":"1机器学习/1算法原理/4深度学习/2lossfunction/crossEntropy.md","localizedDate":"2025年2月25日","excerpt":"<h1> crossEntropy</h1>\\n<p>Categorical Cross-Entropy Loss Function applies the Softmax Activation Function to a model's output (logits) before applying the Negative Log-Likelihood function.</p>\\n<p class=\\"katex-block\\"><span class=\\"katex-display\\"><span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\" display=\\"block\\"><semantics><mrow><mi>P</mi><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy=\\"false\\">(</mo><mi>O</mi><mo stretchy=\\"false\\">)</mo></mrow><annotation encoding=\\"application/x-tex\\">\\nP = softmax(O)\\n</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6833em;\\"></span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.13889em;\\">P</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span><span class=\\"mrel\\">=</span><span class=\\"mspace\\" style=\\"margin-right:0.2778em;\\"></span></span><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:1em;vertical-align:-0.25em;\\"></span><span class=\\"mord mathnormal\\">so</span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.10764em;\\">f</span><span class=\\"mord mathnormal\\">t</span><span class=\\"mord mathnormal\\">ma</span><span class=\\"mord mathnormal\\">x</span><span class=\\"mopen\\">(</span><span class=\\"mord mathnormal\\" style=\\"margin-right:0.02778em;\\">O</span><span class=\\"mclose\\">)</span></span></span></span></span></p>","copyright":{"author":"王新宇"},"autoDesc":true}`);export{t as data};
