const e=JSON.parse('{"key":"v-70d679ec","path":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/nlp_seq2seq.html","title":"seq2seq","lang":"zh-CN","frontmatter":{"description":"seq2seq seq2seq模型也称为Encoder-Decoder模型, 既该模型包含了Encoder(编码器)和Decoder(解码器). 考虑将日语翻译成英语, 如下图: 首先对\\"吾輩は猫である\\"这句话进行编码, 然后将编码好的信息传递给解码器, 由解码器生成目标文本. 编码器利用LSTM将时序数据转换为隐状态hhh.隐状态hhh是LSTM层的最后一个隐状态, 其中编码了翻译输入文本所需的信息.隐状态hhh是一个固定长度的向量. 解码器的结构和前面的神经网络相同, 不过也有稍许的差异, 就是LSTM会接收向量hhh. 在前面的神经网络中, LSTM层不接收任何信息(接收初始化为0的向量).","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/nlp_seq2seq.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:title","content":"seq2seq"}],["meta",{"property":"og:description","content":"seq2seq seq2seq模型也称为Encoder-Decoder模型, 既该模型包含了Encoder(编码器)和Decoder(解码器). 考虑将日语翻译成英语, 如下图: 首先对\\"吾輩は猫である\\"这句话进行编码, 然后将编码好的信息传递给解码器, 由解码器生成目标文本. 编码器利用LSTM将时序数据转换为隐状态hhh.隐状态hhh是LSTM层的最后一个隐状态, 其中编码了翻译输入文本所需的信息.隐状态hhh是一个固定长度的向量. 解码器的结构和前面的神经网络相同, 不过也有稍许的差异, 就是LSTM会接收向量hhh. 在前面的神经网络中, LSTM层不接收任何信息(接收初始化为0的向量)."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://claroja.github.io/"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-25T13:46:58.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:alt","content":"seq2seq"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2023-11-25T13:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"seq2seq\\",\\"image\\":[\\"https://claroja.github.io/\\"],\\"dateModified\\":\\"2023-11-25T13:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"应用","slug":"应用","link":"#应用","children":[]},{"level":2,"title":"seq2seq的实现","slug":"seq2seq的实现","link":"#seq2seq的实现","children":[{"level":3,"title":"Encoder类","slug":"encoder类","link":"#encoder类","children":[]}]},{"level":2,"title":"Decoder类","slug":"decoder类","link":"#decoder类","children":[]},{"level":2,"title":"反转输入数据(Reverse)","slug":"反转输入数据-reverse","link":"#反转输入数据-reverse","children":[]},{"level":2,"title":"偷窥(Peeky)","slug":"偷窥-peeky","link":"#偷窥-peeky","children":[]},{"level":2,"title":"seq2seq的应用","slug":"seq2seq的应用","link":"#seq2seq的应用","children":[]}],"git":{"createdTime":1700920018000,"updatedTime":1700920018000,"contributors":[{"name":"claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":9.21,"words":2764},"filePathRelative":"机器学习/自然语言处理/自然语言处理/nlp_seq2seq.md","localizedDate":"2023年11月25日","excerpt":"<h1> seq2seq</h1>\\n<p>seq2seq模型也称为Encoder-Decoder模型, 既该模型包含了Encoder(编码器)和Decoder(解码器).</p>\\n<p>考虑将日语翻译成英语, 如下图:\\n\\n首先对\\"吾輩は猫である\\"这句话进行编码, 然后将编码好的信息传递给解码器, 由解码器生成目标文本.\\n\\n编码器利用LSTM将时序数据转换为隐状态<span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><mi>h</mi></mrow><annotation encoding=\\"application/x-tex\\">h</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6944em;\\"></span><span class=\\"mord mathnormal\\">h</span></span></span></span>.隐状态<span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><mi>h</mi></mrow><annotation encoding=\\"application/x-tex\\">h</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6944em;\\"></span><span class=\\"mord mathnormal\\">h</span></span></span></span>是LSTM层的最后一个隐状态, 其中编码了翻译输入文本所需的信息.隐状态<span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><mi>h</mi></mrow><annotation encoding=\\"application/x-tex\\">h</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6944em;\\"></span><span class=\\"mord mathnormal\\">h</span></span></span></span>是一个固定长度的向量.\\n\\n解码器的结构和前面的神经网络相同, 不过也有稍许的差异, 就是LSTM会接收向量<span class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><mi>h</mi></mrow><annotation encoding=\\"application/x-tex\\">h</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.6944em;\\"></span><span class=\\"mord mathnormal\\">h</span></span></span></span>. 在前面的神经网络中, LSTM层不接收任何信息(接收初始化为0的向量).</p>","copyright":{"author":"王新宇"},"autoDesc":true}');export{e as data};
