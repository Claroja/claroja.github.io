const n=JSON.parse(`{"key":"v-f704051a","path":"/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/3%E6%A0%91%E6%A8%A1%E5%9E%8B/7LightGBM/7LightGBM%E5%AE%9E%E8%B7%B5.html","title":"","lang":"zh-CN","frontmatter":{"description":"API LGBMModel是LGBMClassifier, LGBMRegressor,LGBMRanker的父类, 不建议直接使用. 而建议使用LGBMClassifier, LGBMRegressor,LGBMRanker, 因为他们提供了合适的损失函数, 以及合适的输出, 比如而建议使用LGBMClassifier可以输出概率. LGBMClassifier 构造参数 lightgbm.LGBMClassifier( boosting_type='gbdt', # str, ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘rf’, Random Forest. num_leaves=31, # int, 每棵基树最大叶子节点个数 max_depth=-1, # int, 每棵基树的深度 learning_rate=0.1, # float, 学习速率 n_estimators=100, # int, 子树的个数 subsample_for_bin=200000, # int, 构建列直方图时，每个特征都会被分箱，此参数控制单个特征的单个箱子所能容纳的最多样本数。 objective=None, # str, 目标类型, Default: ‘regression’ for LGBMRegressor, ‘binary’ or ‘multiclass’ for LGBMClassifier, ‘lambdarank’ for LGBMRanker. class_weight=None, # dict, 'balanced' or None, balanced则自动，比如三类，0类10个，1类90个，2类900个，则自动将0类权重设为1000/10=100,1类为1000/90,2类为1000/900,很大的系数，会被优先照顾，一般设置了权重，则模型会重点找全这个类别，但隐患是可能会增加误判，简称召回棒棒哒、精确率血崩。如果传字典就要一一对应，比如{'0':100,'1':11,'2':1.1}，默认无，大家都是1。 min_split_gain=0.0, # float, 分裂要求最小的增益 min_child_weight=0.001, # float, 叶子节点要求的样本最小权重之和 min_child_samples=20, # int, 叶子节点要求样本最小个数 subsample=1.0, # float, 行采样比例，0-1之间，一般设个0.6~0.8左右，有助于加快速度和控制过拟合，也是不放回抽样。 subsample_freq=0, # int, 0表示不适用bagging采样, 正数表示没k个迭代 做一次bagging采样. colsample_bytree=1.0, # float, 每棵基树的列采样比例 reg_alpha=0.0, # float, L1正则 reg_lambda=0.0, # float, L2正则 random_state=None, # int, 随机种子 n_jobs=None, # int or None, 并行度 importance_type='split', # str, 特征重要性计算方式，一个是算用到过多少次，一个是算分裂带来的增益，树模型在此处有个问题，就是一个特征一旦被使用，特别是第一个，往往会造成很高的增益，比如feature-A实际上只比feature-B，划分增益高一点，但模型肯定选了A，那么下一步B带来的增益会减很多，实际上B只比A稍微弱一点，但算gain的话，B会比A少很多，虽然随机抽列会降低这个影响，但并不能完全消除。 ) 实例方法 fit(X, y[, sample_weight, init_score, ...]) get_params([deep]) predict(X[, raw_score, start_iteration, ...]) predict_proba(X[, raw_score, ...]) score(X, y[, sample_weight]) 实例属性 best_iteration_: 迭代中最好表现的模型, 前提要设置early_stopping()回调函数 classes_: 目标数组 feature_importances_: 特征重要程度 feature_name_: 特征数组 n_classes_: 目标个数 n_estimators_: 模型个数 n_features_: 特征个数 n_features_in_: n_iter_: 迭代次数 fit(X, y[, sample_weight, init_score, ...]) fit( X, # numpy array, pandas DataFrame, scipy.sparse, 二维数组[n_samples, n_features], 元素是int或者float y, # numpy array, pandas DataFrame or Series, [n_samples], 元素是int或float sample_weight=None, # numpy array, pandas Series, [n_samples], 元素是int或float, 或者为None init_score=None, # 训练集样本初始得分 eval_set=None, # list or None, A list of (X, y) tuple, 作为验证集 eval_names=None, # list of str, or None, 验证集的名字 eval_sample_weight=None, # 验证集样本权重 eval_class_weight=None, # 验证集特征权重 eval_init_score=None, # 验证集样本初始得分 eval_metric=None, # str, callable, list or None, 评估模型的好坏的指标, 默认‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker. feature_name='auto', # list of str, or 'auto', 特征名字, 'auto'表示使用dataframe的列名 categorical_feature='auto', # list of str or int, or 'auto', 如果是'auto'则会自动检测并使用dataframe的unordered categorical columns. 建议从0开始编码, float将会直接取整, 负数当成缺失值处理. callbacks=None, # list of callable, or None, 每次迭代后调用 init_model=None # str, pathlib.Path, Booster, LGBMModel or None, 使用预训练的模型 )","head":[["meta",{"property":"og:url","content":"https://claroja.github.io/2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/1%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86/3%E6%A0%91%E6%A8%A1%E5%9E%8B/7LightGBM/7LightGBM%E5%AE%9E%E8%B7%B5.html"}],["meta",{"property":"og:site_name","content":"Claroja"}],["meta",{"property":"og:description","content":"API LGBMModel是LGBMClassifier, LGBMRegressor,LGBMRanker的父类, 不建议直接使用. 而建议使用LGBMClassifier, LGBMRegressor,LGBMRanker, 因为他们提供了合适的损失函数, 以及合适的输出, 比如而建议使用LGBMClassifier可以输出概率. LGBMClassifier 构造参数 lightgbm.LGBMClassifier( boosting_type='gbdt', # str, ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘rf’, Random Forest. num_leaves=31, # int, 每棵基树最大叶子节点个数 max_depth=-1, # int, 每棵基树的深度 learning_rate=0.1, # float, 学习速率 n_estimators=100, # int, 子树的个数 subsample_for_bin=200000, # int, 构建列直方图时，每个特征都会被分箱，此参数控制单个特征的单个箱子所能容纳的最多样本数。 objective=None, # str, 目标类型, Default: ‘regression’ for LGBMRegressor, ‘binary’ or ‘multiclass’ for LGBMClassifier, ‘lambdarank’ for LGBMRanker. class_weight=None, # dict, 'balanced' or None, balanced则自动，比如三类，0类10个，1类90个，2类900个，则自动将0类权重设为1000/10=100,1类为1000/90,2类为1000/900,很大的系数，会被优先照顾，一般设置了权重，则模型会重点找全这个类别，但隐患是可能会增加误判，简称召回棒棒哒、精确率血崩。如果传字典就要一一对应，比如{'0':100,'1':11,'2':1.1}，默认无，大家都是1。 min_split_gain=0.0, # float, 分裂要求最小的增益 min_child_weight=0.001, # float, 叶子节点要求的样本最小权重之和 min_child_samples=20, # int, 叶子节点要求样本最小个数 subsample=1.0, # float, 行采样比例，0-1之间，一般设个0.6~0.8左右，有助于加快速度和控制过拟合，也是不放回抽样。 subsample_freq=0, # int, 0表示不适用bagging采样, 正数表示没k个迭代 做一次bagging采样. colsample_bytree=1.0, # float, 每棵基树的列采样比例 reg_alpha=0.0, # float, L1正则 reg_lambda=0.0, # float, L2正则 random_state=None, # int, 随机种子 n_jobs=None, # int or None, 并行度 importance_type='split', # str, 特征重要性计算方式，一个是算用到过多少次，一个是算分裂带来的增益，树模型在此处有个问题，就是一个特征一旦被使用，特别是第一个，往往会造成很高的增益，比如feature-A实际上只比feature-B，划分增益高一点，但模型肯定选了A，那么下一步B带来的增益会减很多，实际上B只比A稍微弱一点，但算gain的话，B会比A少很多，虽然随机抽列会降低这个影响，但并不能完全消除。 ) 实例方法 fit(X, y[, sample_weight, init_score, ...]) get_params([deep]) predict(X[, raw_score, start_iteration, ...]) predict_proba(X[, raw_score, ...]) score(X, y[, sample_weight]) 实例属性 best_iteration_: 迭代中最好表现的模型, 前提要设置early_stopping()回调函数 classes_: 目标数组 feature_importances_: 特征重要程度 feature_name_: 特征数组 n_classes_: 目标个数 n_estimators_: 模型个数 n_features_: 特征个数 n_features_in_: n_iter_: 迭代次数 fit(X, y[, sample_weight, init_score, ...]) fit( X, # numpy array, pandas DataFrame, scipy.sparse, 二维数组[n_samples, n_features], 元素是int或者float y, # numpy array, pandas DataFrame or Series, [n_samples], 元素是int或float sample_weight=None, # numpy array, pandas Series, [n_samples], 元素是int或float, 或者为None init_score=None, # 训练集样本初始得分 eval_set=None, # list or None, A list of (X, y) tuple, 作为验证集 eval_names=None, # list of str, or None, 验证集的名字 eval_sample_weight=None, # 验证集样本权重 eval_class_weight=None, # 验证集特征权重 eval_init_score=None, # 验证集样本初始得分 eval_metric=None, # str, callable, list or None, 评估模型的好坏的指标, 默认‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker. feature_name='auto', # list of str, or 'auto', 特征名字, 'auto'表示使用dataframe的列名 categorical_feature='auto', # list of str or int, or 'auto', 如果是'auto'则会自动检测并使用dataframe的unordered categorical columns. 建议从0开始编码, float将会直接取整, 负数当成缺失值处理. callbacks=None, # list of callable, or None, 每次迭代后调用 init_model=None # str, pathlib.Path, Booster, LGBMModel or None, 使用预训练的模型 )"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-24T12:46:58.000Z"}],["meta",{"property":"article:author","content":"claroja"}],["meta",{"property":"article:modified_time","content":"2025-02-24T12:46:58.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-24T12:46:58.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"claroja\\",\\"url\\":\\"https://claroja.github.io\\"}]}"]]},"headers":[{"level":2,"title":"API","slug":"api","link":"#api","children":[{"level":3,"title":"LGBMClassifier","slug":"lgbmclassifier","link":"#lgbmclassifier","children":[]},{"level":3,"title":"boosting_type='gbdt'","slug":"boosting-type-gbdt","link":"#boosting-type-gbdt","children":[]}]},{"level":2,"title":"超参调整","slug":"超参调整","link":"#超参调整","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1740401218000,"updatedTime":1740401218000,"contributors":[{"name":"Claroja","email":"63183535@qq.com","commits":1}]},"readingTime":{"minutes":7.7,"words":2311},"filePathRelative":"2机器学习/1算法原理/3树模型/7LightGBM/7LightGBM实践.md","localizedDate":"2025年2月24日","excerpt":"<h2> API</h2>\\n<p>LGBMModel是LGBMClassifier, LGBMRegressor,LGBMRanker的父类, 不建议直接使用. 而建议使用LGBMClassifier, LGBMRegressor,LGBMRanker, 因为他们提供了合适的损失函数, 以及合适的输出, 比如而建议使用LGBMClassifier可以输出概率.</p>\\n<h3> LGBMClassifier</h3>\\n<ol>\\n<li>\\n<p>构造参数</p>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code>lightgbm<span class=\\"token punctuation\\">.</span>LGBMClassifier<span class=\\"token punctuation\\">(</span>\\n    boosting_type<span class=\\"token operator\\">=</span><span class=\\"token string\\">'gbdt'</span><span class=\\"token punctuation\\">,</span>           <span class=\\"token comment\\"># str, ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘rf’, Random Forest.</span>\\n    num_leaves<span class=\\"token operator\\">=</span><span class=\\"token number\\">31</span><span class=\\"token punctuation\\">,</span>                  <span class=\\"token comment\\"># int, 每棵基树最大叶子节点个数</span>\\n    max_depth<span class=\\"token operator\\">=</span><span class=\\"token operator\\">-</span><span class=\\"token number\\">1</span><span class=\\"token punctuation\\">,</span>                   <span class=\\"token comment\\"># int, 每棵基树的深度</span>\\n    learning_rate<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.1</span><span class=\\"token punctuation\\">,</span>              <span class=\\"token comment\\"># float, 学习速率</span>\\n    n_estimators<span class=\\"token operator\\">=</span><span class=\\"token number\\">100</span><span class=\\"token punctuation\\">,</span>               <span class=\\"token comment\\"># int, 子树的个数</span>\\n    subsample_for_bin<span class=\\"token operator\\">=</span><span class=\\"token number\\">200000</span><span class=\\"token punctuation\\">,</span>       <span class=\\"token comment\\"># int, 构建列直方图时，每个特征都会被分箱，此参数控制单个特征的单个箱子所能容纳的最多样本数。</span>\\n    objective<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>                 <span class=\\"token comment\\"># str, 目标类型, Default: ‘regression’ for LGBMRegressor, ‘binary’ or ‘multiclass’ for LGBMClassifier, ‘lambdarank’ for LGBMRanker.</span>\\n    class_weight<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>              <span class=\\"token comment\\"># dict, 'balanced' or None, balanced则自动，比如三类，0类10个，1类90个，2类900个，则自动将0类权重设为1000/10=100,1类为1000/90,2类为1000/900,很大的系数，会被优先照顾，一般设置了权重，则模型会重点找全这个类别，但隐患是可能会增加误判，简称召回棒棒哒、精确率血崩。如果传字典就要一一对应，比如{'0':100,'1':11,'2':1.1}，默认无，大家都是1。</span>\\n    min_split_gain<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.0</span><span class=\\"token punctuation\\">,</span>             <span class=\\"token comment\\"># float, 分裂要求最小的增益</span>\\n    min_child_weight<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.001</span><span class=\\"token punctuation\\">,</span>         <span class=\\"token comment\\"># float, 叶子节点要求的样本最小权重之和</span>\\n    min_child_samples<span class=\\"token operator\\">=</span><span class=\\"token number\\">20</span><span class=\\"token punctuation\\">,</span>           <span class=\\"token comment\\"># int, 叶子节点要求样本最小个数</span>\\n    subsample<span class=\\"token operator\\">=</span><span class=\\"token number\\">1.0</span><span class=\\"token punctuation\\">,</span>                  <span class=\\"token comment\\"># float, 行采样比例，0-1之间，一般设个0.6~0.8左右，有助于加快速度和控制过拟合，也是不放回抽样。</span>\\n    subsample_freq<span class=\\"token operator\\">=</span><span class=\\"token number\\">0</span><span class=\\"token punctuation\\">,</span>               <span class=\\"token comment\\"># int, 0表示不适用bagging采样, 正数表示没k个迭代 做一次bagging采样.</span>\\n    colsample_bytree<span class=\\"token operator\\">=</span><span class=\\"token number\\">1.0</span><span class=\\"token punctuation\\">,</span>           <span class=\\"token comment\\"># float, 每棵基树的列采样比例</span>\\n    reg_alpha<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.0</span><span class=\\"token punctuation\\">,</span>                  <span class=\\"token comment\\"># float, L1正则</span>\\n    reg_lambda<span class=\\"token operator\\">=</span><span class=\\"token number\\">0.0</span><span class=\\"token punctuation\\">,</span>                 <span class=\\"token comment\\"># float, L2正则</span>\\n    random_state<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>              <span class=\\"token comment\\"># int, 随机种子</span>\\n    n_jobs<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>                    <span class=\\"token comment\\"># int or None, 并行度</span>\\n    importance_type<span class=\\"token operator\\">=</span><span class=\\"token string\\">'split'</span><span class=\\"token punctuation\\">,</span>        <span class=\\"token comment\\"># str, 特征重要性计算方式，一个是算用到过多少次，一个是算分裂带来的增益，树模型在此处有个问题，就是一个特征一旦被使用，特别是第一个，往往会造成很高的增益，比如feature-A实际上只比feature-B，划分增益高一点，但模型肯定选了A，那么下一步B带来的增益会减很多，实际上B只比A稍微弱一点，但算gain的话，B会比A少很多，虽然随机抽列会降低这个影响，但并不能完全消除。</span>\\n<span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div></li>\\n<li>\\n<p>实例方法</p>\\n<ol>\\n<li>fit(X, y[, sample_weight, init_score, ...])</li>\\n<li>get_params([deep])</li>\\n<li>predict(X[, raw_score, start_iteration, ...])</li>\\n<li>predict_proba(X[, raw_score, ...])</li>\\n<li>score(X, y[, sample_weight])</li>\\n</ol>\\n</li>\\n<li>\\n<p>实例属性</p>\\n<ol>\\n<li>best_iteration_: 迭代中最好表现的模型, 前提要设置early_stopping()回调函数</li>\\n<li>classes_: 目标数组</li>\\n<li>feature_importances_: 特征重要程度</li>\\n<li>feature_name_: 特征数组</li>\\n<li>n_classes_: 目标个数</li>\\n<li>n_estimators_: 模型个数</li>\\n<li>n_features_: 特征个数</li>\\n<li>n_features_in_:</li>\\n<li>n_iter_: 迭代次数</li>\\n</ol>\\n</li>\\n<li>\\n<p>fit(X, y[, sample_weight, init_score, ...])</p>\\n<div class=\\"language-python line-numbers-mode\\" data-ext=\\"py\\"><pre class=\\"language-python\\"><code>fit<span class=\\"token punctuation\\">(</span>\\n    X<span class=\\"token punctuation\\">,</span>                          <span class=\\"token comment\\"># numpy array, pandas DataFrame, scipy.sparse, 二维数组[n_samples, n_features], 元素是int或者float</span>\\n    y<span class=\\"token punctuation\\">,</span>                          <span class=\\"token comment\\"># numpy array, pandas DataFrame or Series, [n_samples], 元素是int或float</span>\\n    sample_weight<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>         <span class=\\"token comment\\"># numpy array, pandas Series, [n_samples], 元素是int或float, 或者为None</span>\\n    init_score<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>            <span class=\\"token comment\\"># 训练集样本初始得分</span>\\n    eval_set<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>              <span class=\\"token comment\\"># list or None, A list of (X, y) tuple, 作为验证集</span>\\n    eval_names<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>            <span class=\\"token comment\\"># list of str, or None, 验证集的名字</span>\\n    eval_sample_weight<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>    <span class=\\"token comment\\"># 验证集样本权重</span>\\n    eval_class_weight<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>     <span class=\\"token comment\\"># 验证集特征权重</span>\\n    eval_init_score<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>       <span class=\\"token comment\\"># 验证集样本初始得分</span>\\n    eval_metric<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>           <span class=\\"token comment\\"># str, callable, list or None, 评估模型的好坏的指标, 默认‘l2’ for LGBMRegressor, ‘logloss’ for LGBMClassifier, ‘ndcg’ for LGBMRanker.</span>\\n    feature_name<span class=\\"token operator\\">=</span><span class=\\"token string\\">'auto'</span><span class=\\"token punctuation\\">,</span>        <span class=\\"token comment\\"># list of str, or 'auto', 特征名字, 'auto'表示使用dataframe的列名</span>\\n    categorical_feature<span class=\\"token operator\\">=</span><span class=\\"token string\\">'auto'</span><span class=\\"token punctuation\\">,</span> <span class=\\"token comment\\"># list of str or int, or 'auto', 如果是'auto'则会自动检测并使用dataframe的unordered categorical columns. 建议从0开始编码, float将会直接取整, 负数当成缺失值处理.</span>\\n    callbacks<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span><span class=\\"token punctuation\\">,</span>             <span class=\\"token comment\\"># list of callable, or None, 每次迭代后调用</span>\\n    init_model<span class=\\"token operator\\">=</span><span class=\\"token boolean\\">None</span>             <span class=\\"token comment\\"># str, pathlib.Path, Booster, LGBMModel or None, 使用预训练的模型</span>\\n<span class=\\"token punctuation\\">)</span>\\n</code></pre><div class=\\"line-numbers\\" aria-hidden=\\"true\\"><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div><div class=\\"line-number\\"></div></div></div></li>\\n</ol>","copyright":{"author":"王新宇"},"autoDesc":true}`);export{n as data};
