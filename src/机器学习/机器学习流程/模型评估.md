# 多维随机变量



## 模型评估
### 基础概念
- P（Positive）：预测为正样本
- N（Negative）：预测为负样本
- T（True）：预测正确
- F（False）：预测错误
- TP: True Positive: P是指预测的是正样本, T表示预测和label一样, 即label也是正样本
- FP: False Positive: P是指预测的是正样本, F表示预测和label相反, 即label是负样本
- TN: True Negative: N是指预测的是负样本, T表示预测和label一样, 即label也是负样本
- FN: False Negative: N是指预测的是负样本, F表示预测和label相反, 即label是正样本

根据以上概念, 可以构建混淆矩阵:

label\predict|Positive|Negative
--|--|--
Positive|TP(真阳)|FN(伪阴)
Negative|FP(伪阳)|TN(真阳)
### 指标
#### Accuracy
$$accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$
accuracy就是预测正确的结果占总样本的百分比.
缺点: 在样本不均衡的情况下, 不能作为很好的指标来衡量结果. 比如在一个总样本中, 正样本占比90%, 负样本占比10%. 即使负样本全部预测错, 最后的整体准确率也是90%.

#### Precision
$$precision = \frac{TP}{TP+FP}$$
precision就是在全部被预测为正样本(P)的数据中, 有多少label是正样本(TP). 通俗就是模型预测出的正样本的准确率是多少, 既查准率.

#### Recall
$$recall = \frac{TP}{TP+FN}$$
recall就是在全部lable为正样本(TP+FN)数据中, 有多少label是正样本(TP). 通俗就是模型发现的正样本, 占所有正样本的比例, 既查全率.

Precision-Recall Curve
[1.png](1.png)
https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html


#### F1 Score
F1 Score是recall和precision的调和平均数.
$$F_1=\frac2{recall^{-1}+precision^{-1}}=2\cdot \frac{precision\cdot recall}{precision+recall}$$
更通用的的F Score是$$F_\beta$$,添加了一个因子$$\beta$$,含义是recall的权重是precision的$$\beta$$倍:
1. $$\beta=2$$recall的重要性是precision的2倍
2. $$\beta=0.5$$recall的重要性是precision的0.5倍
$$F_\beta=\frac{1+\beta^2}{\frac{\beta^2}{recall}+\frac{1}{precision}}=(1+\beta^2)\cdot\frac{precision\cdot recall}{(\beta^2\cdot precision)+recall}$$

https://en.wikipedia.org/wiki/F-score


#### ROC
ROC(Receiver operating characteristic)是一个二分类模型评估方法.名字的来源是其最早发明用于军队雷达接收的操作者(operators of military radar receivers)
1. 伪阳率(FPR):在label为阴性的样本中, 被错误地判断为阳性的比率.$$FPR=\frac{FP}{FP+TN}$$
假阳率, 即误伤率 ,将阴性的样本判别为阳性.
2. 真阳率(TPR):在label为阳性的样本中, 被正确地判断为阳性的比率. $$TPR=\frac{TP}{TP+FN}$$
真阳率即我们所说的召回率(recall)

比如在作弊审核中.尽量把作弊玩家审核出来是主要任务, 也就是TPR越高越好.而把正常玩家, 误审为作弊玩家, 也就是FPR, 要越低越好. 两个指标是相互制约的. 如果我们比较在意TPR, 即想把所有作弊玩家全部揪出来, 那么不可避免的FPR也会相应升高, 即会误伤很多正常的玩家. 极端的, 把所有的玩家都预测为作弊玩家, 那么TPR为1, FPR也为1.


ROC空间将伪阳率(FPR)定义为X轴, 真阳率(TPR)定义为Y轴:
[2.png](2.png)
1. 图片中的左上角(TPR=1,FPR=0),为完美分类, 即作弊全部预测正确, 且没有误伤
2. 图片中的红色线上的点(TPR=FPR), 即作弊揪出50%, 误伤50%. 可以说是蒙对一半, 蒙错一半.
3. 图片中的右下角(TPR=0,FPR=1), 将作弊的全部判为正常, 将正常的全部判为作弊.

ROC曲线, 只能在某个特定阈值来评价模型, 我们需要一个独立于阈值的评价指标来衡量这个模型如何, 也就是遍历所有阈值, 得到ROC曲线.
[3.png](3.png)
上图中, 我们可以根据需要来选择不同的TPR和FPR.
[4.png](4.png)
上图中, 我们可以以求均值的方式对多分类模型进行评估
#### AUC
AUC(Area Under Curve)值为ROC曲线下面区域面积，显然，AUC越大，分类器分类效果越好。
1. AUC = 1，不管设定什么阈值都能得出完美预测
2. 0.5 < AUC < 1, 妥善设定阈值的话，能有预测价值
3. AUC = 0.5, 跟随机猜测一样（例：丢铜板），模型没有预测价值。
4. AUC < 0.5，比随机猜测还差, 但只要总是反预测而行，就优于随机猜测。


如何记忆上述指标
重要的是记住混淆矩阵
1. axis=0, 即按列来看,Positive列之和是预测为正样本的总数.
$$precision=\frac{TP}{TP+FP}=\frac{TP}{P_{column}}$$
[5.png](5.png)
2. axis=1, 即按行来看, Positive行之和是实际为正样本的总数.
$$TPR(recall)=\frac{TP}{TP+FN}=\frac{TP}{P_{row}}$$
[6.png](6.png)
3. axis=1, 即按行来看, Negative之和是实际为负样本的总数.
$$FPR=\frac{FP}{FP+TN}=\frac{FP}{N_{row}}$$
[7.png](7.png)
4. 从整体来看
$$accuracy=\frac{TP+TN}{TP+TN+FP+FN}=\frac{T}{T+F}$$

[ROC曲线与AUC值 - gatherstars - 博客园](https://www.cnblogs.com/gatherstars/p/6084696.html)
[Receiver Operating Characteristic (ROC) ‒ scikit-learn 0.24.2 documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html)
[【机器学习笔记】：一文让你彻底理解准确率，精准率，召回率，真正率，假正率，ROC/AUC](https://zhuanlan.zhihu.com/p/46714763)

